{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 409,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate as sci\n",
    "import time\n",
    "from getdist import plots, MCSamples\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "ds = tf.contrib.distributions\n",
    "\n",
    "%matplotlib inline\n",
    "dpi = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved things directory\n",
    "direc = '/home/mauricio/Documents/Uni/Intro_2/' + 'gal.txt'\n",
    "\n",
    "# Carga de datos\n",
    "redshift = np.genfromtxt('gal.txt', usecols=(1))\n",
    "mu_obs = np.genfromtxt('gal.txt', usecols=(2)) # m - M\n",
    "cov = np.genfromtxt('gal.txt', usecols=(3))\n",
    "\n",
    "p = np.argsort(redshift)\n",
    "redshift = redshift[p].astype(np.float32)\n",
    "mu_obs = mu_obs[p]\n",
    "cov = cov[p]\n",
    "cov = np.diag(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def f1(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300), axis=1)\n",
    "    o_k_s = tf.reshape(tf.sqrt(abs(omk)), [batch_size, 1])\n",
    "    return (1 + z)*tf.sinh(o_k_s*I)/(o_k_s + 1e-300)\n",
    "\n",
    "\n",
    "def f2(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300), axis=1)\n",
    "    o_k_s = tf.reshape(tf.sqrt(abs(omk)), [batch_size, 1])\n",
    "    return (1 + z)*tf.sin(o_k_s*I)/(o_k_s + 1e-300)\n",
    "\n",
    "\n",
    "def f3(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300), axis=1)\n",
    "    return (1 + z)*I\n",
    "\n",
    "\n",
    "def EHubble(theta, z): # parametro de hubble\n",
    "    \"\"\"\n",
    "    theta: parameter space state.\n",
    "    z: redshift.\n",
    "    bs: batch size.\n",
    "    \"\"\"\n",
    "    bs = batch_size\n",
    "    om0 = theta[:, 0]\n",
    "    ol = theta[:, 1]\n",
    "    w = theta[:, 2]\n",
    "    ts = tf.shape(theta)\n",
    "    zz = np.tile(z, (bs, 1))\n",
    "    arg = tf.reshape(om0, [ts[0], 1])*(1 + z)**3 + tf.reshape((1 - om0 - ol), [ts[0], 1])*(1 + z)**2 + tf.reshape(ol, [ts[0], 1])*(1 + z)**(3*(1 + tf.reshape(w, [ts[0], 1])))\n",
    "    EE = tf.sqrt(arg)\n",
    "    return EE, arg\n",
    "\n",
    "\n",
    "def modelo(theta, z):    \n",
    "    om0 = theta[:, 0]\n",
    "    ol = theta[:, 1]\n",
    "    w = theta[:, 2]    \n",
    "    omega_k = 1 - om0 - ol\n",
    "    sig = tf.sign(omega_k)\n",
    "    may = tf.reshape(1 + tf.sign(sig - 1), [batch_size, 1])\n",
    "    men = tf.reshape(1 - tf.abs(sig), [batch_size, 1])\n",
    "    eq = tf.reshape(1 - tf.sign(sig + 1), [batch_size, 1])    \n",
    "    dl = may*f1(theta, z, omega_k) + eq*f3(theta, z, omega_k) + men*f2(theta, z, omega_k)\n",
    "    # integral\n",
    "    dist = 5*tf.log(dl + 1e-300)/np.log(10)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class potential:\n",
    "    def __init__(self, dat, sigma, z):\n",
    "        self.data = dat\n",
    "        self.cov = sigma\n",
    "        self.z = z\n",
    "    \n",
    "    def value(self, theta):\n",
    "        self.mod = modelo(theta, self.z)\n",
    "        self.u = - likelihood(self.mod, self.data, self.cov) #- prior(theta, ndim) \n",
    "        return self.u\n",
    "    \n",
    "    def grad(self, theta):\n",
    "        self.mod = modelo(theta, self.z)\n",
    "        self.u = - likelihood(self.mod, self.data, self.cov)\n",
    "        self.gradient = tf.gradients(self.u, theta)\n",
    "        return self.gradient\n",
    "\n",
    "\n",
    "def likelihood(mod, dat, sigma): # retorna escalar, log(L)\n",
    "    \"\"\"Log likelihood\n",
    "    mod: tf.tensor with model results\n",
    "    dat: numpy array of data\n",
    "    sigma: numpy array of covariance\"\"\"\n",
    "    sig = tf.cast(tf.diag_part(sigma), tf.float32)\n",
    "    L = -0.5*chi2(mod, dat, sigma)[0]  + tf.reduce_sum(-0.5*tf.log(2*np.pi*sig**2))\n",
    "    return L\n",
    "\n",
    "\n",
    "def chi2(mod, dat, sigma):\n",
    "    dat1 = np.tile(dat, (batch_size, 1))\n",
    "    sig = tf.cast(tf.diag_part(sigma), tf.float32)\n",
    "    sig1 = np.tile(sig, (batch_size, 1))\n",
    "    AA = tf.reduce_sum(tf.square((dat1 - mod)/sig))\n",
    "    BB = tf.reduce_sum((dat1 - mod)/tf.square(sig))\n",
    "    CC = tf.reduce_sum(1/tf.square(sig))\n",
    "    chi = AA - (BB**2)/CC\n",
    "    return chi, BB/CC\n",
    "\n",
    "\n",
    "def time_encodig(m, m_act):\n",
    "    arg = 2*np.pi*m_act/m\n",
    "    val = np.array([np.cos(arg), np.sin(arg)])\n",
    "    return tf.assign(t1, val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 412,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior:\n",
    "    def __init__(self, name, low=None, high=None, mean=None, cov=None):\n",
    "        if name=='uniform':\n",
    "            self.u = tf.distributions.Uniform(low=low, high=high)\n",
    "        elif name=='normal':\n",
    "            self.u = tf.contrib.distributions.MultivariateNormalFullCovariance(loc=mean, covariance_matrix=cov)\n",
    "    \n",
    "    def get_samples(self, n):\n",
    "        return self.u.sample(sample_shape=(n))\n",
    "    \n",
    "    def get_pdf(self, value):\n",
    "        return self.u.prob(value)\n",
    "    \n",
    "    def get_log_pdf(self, value):\n",
    "        return self.u.log_prob(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 413,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leapfrog:\n",
    "    def __init__(self, U, m, ndim, nnx, nnv, e, b):\n",
    "        \"\"\"\n",
    "        U: potential energy function\n",
    "        m: # of leapfrog steps\n",
    "        ndim: # of dimensions\n",
    "        nnx: neural network of x's\n",
    "        nnv: neural network of v's\n",
    "        e: leapfrog step parameter\n",
    "        b: batch size\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.m = m\n",
    "        self.ndim = ndim\n",
    "        self.nnx = nnx\n",
    "        self.nnv = nnv\n",
    "        self.e = e\n",
    "        self.b = b\n",
    "        self.x = tf.Variable(tf.random_normal([b, ndim]))\n",
    "        self.v = tf.Variable(tf.random_normal([b, ndim]))\n",
    "        \n",
    "    def direction(self):\n",
    "        \"\"\"Samples a new direction\"\"\"\n",
    "        self.d = np.random.choice([-1, 1])\n",
    "        \n",
    "    def for_dyn(self):\n",
    "        \"\"\"One step of forward dynamics d=1.\n",
    "        \"\"\"\n",
    "        # remember S, Q, T update in each sub iteration\n",
    "        \"\"\"\n",
    "        self.v = self.v*tf.exp(0.5*self.nnv.S*self.e) - 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        self.x = self.x*tf.exp(self.e*self.nnx.S) + self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)\n",
    "        self.x = tf.squeeze(self.x)\n",
    "        self.v = tf.squeeze(self.v)        \n",
    "        self.v = self.v*tf.exp(0.5*self.nnv.S*self.e) - 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        \"\"\"\n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(0.5*self.nnv.S*self.e) - \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "        tf.assign(self.x, \n",
    "                  tf.squeeze(self.x*tf.exp(self.e*self.nnx.S) + \n",
    "                             self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)))      \n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(0.5*self.nnv.S*self.e) - \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "    \n",
    "    def back_dyn(self):\n",
    "        \"\"\"One step of backward dynamics d=-1.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.v = self.v*tf.exp(- 0.5*self.nnv.S*self.e) + 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        self.x = self.x*tf.exp(- self.e*self.nnx.S) - self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)\n",
    "        self.x = tf.squeeze(self.x)\n",
    "        self.v = tf.squeeze(self.v) \n",
    "        self.v = self.v*tf.exp(- 0.5*self.nnv.S*self.e) + 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        \"\"\"\n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(- 0.5*self.nnv.S*self.e) + \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "        tf.assign(self.x, \n",
    "                  tf.squeeze(self.x*tf.exp(- self.e*self.nnx.S) - self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)))\n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(- 0.5*self.nnv.S*self.e) + \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "        \n",
    "    def dyn(self):\n",
    "        \"\"\"m steps dynamics\"\"\"\n",
    "        #self.direction()\n",
    "        for i in range(self.m):\n",
    "            #self.mask = np.random.choice([1, 0], size=(self.b, self.ndim), p=[0.5, 0.5])\n",
    "            #self.nmask = np.ones((self.b, self.ndim)) - self.mask\n",
    "            if self.d==1:\n",
    "                self.for_dyn()\n",
    "            elif self.d==-1:\n",
    "                self.back_dyn()\n",
    "        \n",
    "    def resample(self):\n",
    "        \"\"\"Resamples velocity and direction\"\"\"\n",
    "        tf.assign(self.v, tf.random_normal(shape=(self.b, self.ndim), \n",
    "                                           mean=0.0, stddev=1.0, dtype=tf.float32))\n",
    "        self.direction()\n",
    "        \n",
    "        \n",
    "    def flip(self):\n",
    "        \"\"\"Flip direction\"\"\"\n",
    "        self.d *= -1      \n",
    "    \n",
    "    def sampling(self):\n",
    "        \"\"\"Sampling operation\"\"\"\n",
    "        self.direction()\n",
    "        self.dyn()\n",
    "        self.flip()\n",
    "    \n",
    "    def update_state(self, x0):\n",
    "        tf.assign(self.x, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2HMC:\n",
    "    def __init__(self, U, prior, n, b, m, lr, sc, reg, lfp, lfq, x0):\n",
    "        \"\"\"\n",
    "        U: energy function\n",
    "        prior: prior distribution\n",
    "        n: # iterations\n",
    "        b: batch size\n",
    "        m: leapfrog steps\n",
    "        lr: learning rate\n",
    "        sc: scale parameter\n",
    "        reg: regularization parameter\n",
    "        lfp: inital samples leapfrog object\n",
    "        lfq: batch samples leapfrog object\n",
    "        x0: initial position in parameter space\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.prior = prior\n",
    "        self.n = n\n",
    "        self.b = b\n",
    "        self.m = m\n",
    "        self.lr = lr\n",
    "        self.sc = sc\n",
    "        self.reg = reg\n",
    "        self.init_samples = self.prior.get_samples(b)\n",
    "        self.lfp = lfp\n",
    "        self.lfq = lfq\n",
    "        self.X = x0 # ep\n",
    "        self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "        \n",
    "    def distance(self):\n",
    "        self.dp = tf.reduce_sum((self.X - self.X1)**2, axis=1)\n",
    "        self.dq = tf.reduce_sum((self.Xq - self.xq)**2, axis=1)\n",
    "    \n",
    "    def acceptance(self):\n",
    "        pass\n",
    "        \n",
    "    def loss(self, d): \n",
    "        return self.sc**2/d - d/self.sc**2\n",
    "        \n",
    "    def Loss(self):\n",
    "        self.distance()\n",
    "        self.acceptance()\n",
    "        self.L0ss = (tf.reduce_mean(self.loss(self.dp)) \n",
    "                     + tf.reduce_mean(self.reg*self.loss(self.dq)))\n",
    "        \n",
    "    def train(self):\n",
    "        self.Loss()\n",
    "        self.optimizer.minimize(self.L0ss)\n",
    "        \n",
    "    def Run(self):\n",
    "        for i in range(self.n):\n",
    "            print(i)\n",
    "            self.xq = self.prior.get_samples(self.b) # from prior distribution eq\n",
    "            # first updates samples state \n",
    "            self.lfp.update_state(self.X) # now leapfrog knows the state to evolve\n",
    "            self.lfp.resample()\n",
    "            self.lfp.sampling()\n",
    "            self.X1 = self.lfp.x\n",
    "            # now updates batch samples state\n",
    "            self.lfq.update_state(self.xq) # now leapfrog knows the state to evolve\n",
    "            self.lfq.resample()\n",
    "            self.lfq.sampling()\n",
    "            self.Xq = self.lfq.x\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 415,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, ndim, n1, n2, ls, lq):\n",
    "        \"\"\"\n",
    "        ndim: # of dimensions\n",
    "        n1: # of neurons of layer 1\n",
    "        ls: output parameter\n",
    "        lq: output parameter\n",
    "        \"\"\"\n",
    "        self.W1 = tf.Variable(tf.random_normal([ndim, n1]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([ndim, n1]))\n",
    "        self.W3 = tf.Variable(tf.random_normal([2, n1])) # time encoding\n",
    "        self.W4 = tf.Variable(tf.random_normal([n1, n2]))\n",
    "        self.Ws = tf.Variable(tf.random_normal([n2, 1]))\n",
    "        self.Wq = tf.Variable(tf.random_normal([n2, 1]))\n",
    "        self.Wt = tf.Variable(tf.random_normal([n1, 1]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([n1]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([n2]))\n",
    "        self.bs = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.bq = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.bt = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.ls = ls\n",
    "        self.lq = lq\n",
    "    \n",
    "    def model(self, x, v, t):\n",
    "        h1 = tf.matmul(x, self.W1) + tf.matmul(v, self.W2) + tf.matmul(t, self.W3) + self.b1\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h2 = tf.matmul(h1, self.W4) + self.b2\n",
    "        h2 = tf.nn.relu(h2)\n",
    "        self.S = tf.tanh(tf.matmul(h2, self.Ws) + self.bs)\n",
    "        self.Q = tf.tanh(tf.matmul(h2, self.Wq) + self.bq)\n",
    "        self.T = tf.matmul(h2, self.Wt) + self.bt\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = [0., 0., -6.]\n",
    "high = [1., 1., 1/3]\n",
    "global batch_size\n",
    "batch_size = 2\n",
    "\n",
    "x1 = np.array([[0.5, 0.4, -0.5], [0.5, 0.4, -0.5]])\n",
    "x1 = tf.convert_to_tensor(x1, dtype=tf.float32)\n",
    "\n",
    "v1 = np.array([[0.3, 0.4, -1.5], [0.3, 0.4, -1.5]])\n",
    "v1 = tf.convert_to_tensor(v1, dtype=tf.float32)\n",
    "\n",
    "t1 = np.zeros((2, 2))\n",
    "t1 = tf.convert_to_tensor(t1, dtype=tf.float32)\n",
    "\n",
    "a1 = np.array([1, 2, 3])\n",
    "a1 = tf.convert_to_tensor(a1, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = potential(mu_obs, cov, redshift)\n",
    "pr = prior(name='uniform', low=low, high=high)\n",
    "mlpx = MLP(ndim=3, n1=10, n2=10, ls=1, lq=1)\n",
    "mlpv = MLP(ndim=3, n1=10, n2=10, ls=1, lq=1)\n",
    "mlpx.model(x1, v1, t1)\n",
    "mlpv.model(x1, v1, t1)\n",
    "lfp = Leapfrog(U=pot, m=2, ndim=3, nnx=mlpx, nnv=mlpv, e=1e-3, b=batch_size)\n",
    "lfq = Leapfrog(U=pot, m=2, ndim=3, nnx=mlpx, nnv=mlpv, e=1e-3, b=batch_size)\n",
    "l2 = L2HMC(pot, pr, 2, batch_size, 2, 1e-3, 1e-1, 1e-1, lfp, lfq, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "[[ 0.5  0.4 -0.5]\n",
      " [ 0.5  0.4 -0.5]]\n",
      "[[ 0.27047983 -0.21588333  0.27633208]\n",
      " [-0.7020332   0.7878616  -0.32845637]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "l2.Run()\n",
    "l2.train()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(l2.X))\n",
    "    print(sess.run(l2.Xq))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.9307843  -1.4582889  -0.28269216 -0.5558202   0.66578835  0.12292377\n",
      "   1.770949   -1.051016    0.6721776   0.11429885]\n",
      " [-0.5223682  -0.19032279 -0.20068412 -2.4377682  -1.9540609  -0.39262894\n",
      "   0.18166028 -1.3761792  -1.683012   -0.39099616]\n",
      " [ 1.2652198   1.4160861   0.6998023   0.23851661 -0.5904471  -0.12717155\n",
      "   0.5946226   1.3062744  -0.00737418 -0.01136716]]\n",
      "[[-0.9307843  -1.4582889  -0.28269216 -0.5558202   0.66578835  0.12292377\n",
      "   1.770949   -1.051016    0.6721776   0.11429885]\n",
      " [-0.5223682  -0.19032279 -0.20068412 -2.4377682  -1.9540609  -0.39262894\n",
      "   0.18166028 -1.3761792  -1.683012   -0.39099616]\n",
      " [ 1.2652198   1.4160861   0.6998023   0.23851661 -0.5904471  -0.12717155\n",
      "   0.5946226   1.3062744  -0.00737418 -0.01136716]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(l2.lfp.nnx.W1))\n",
    "    print(sess.run(l2.lfq.nnx.W1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
