# -*- coding: utf-8 -*-
"""hmc.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1fFSkI8sC3Q2bcSS9FsMRdCAA-qGrwgEp

# Hamiltonian Monte Carlo
"""

import numpy as np
import matplotlib.pyplot as plt
from scipy import integrate as sci
import time
from google.colab import files
import os

import tensorflow as tf
ds = tf.contrib.distributions

# %matplotlib inline

global batch_size
global dpi
dpi = 200
np.set_printoptions(precision=2)

tf.reset_default_graph()

def f1(theta, z, omk):
    zc = np.copy(z)
    zc = np.insert(zc, 0, 0)
    dz = zc[1:] - zc[:-1]
    E = EHubble(theta, z)[0]
    I = tf.cumsum(dz/(E + 1e-300), axis=1)
    o_k_s = tf.reshape(tf.sqrt(abs(omk)), [batch_size, 1])
    return (1 + z)*tf.sinh(o_k_s*I)/(o_k_s + 1e-300)


def f2(theta, z, omk):
    zc = np.copy(z)
    zc = np.insert(zc, 0, 0)
    dz = zc[1:] - zc[:-1]
    E = EHubble(theta, z)[0]
    I = tf.cumsum(dz/(E + 1e-300), axis=1)
    o_k_s = tf.reshape(tf.sqrt(abs(omk)), [batch_size, 1])
    return (1 + z)*tf.sin(o_k_s*I)/(o_k_s + 1e-300)


def f3(theta, z, omk):
    zc = np.copy(z)
    zc = np.insert(zc, 0, 0)
    dz = zc[1:] - zc[:-1]
    E = EHubble(theta, z)[0]
    I = tf.cumsum(dz/(E + 1e-300), axis=1)
    return (1 + z)*I


def EHubble(theta, z): # parametro de hubble
    """
    theta: parameter space state.
    z: redshift.
    bs: batch size.
    """
    bs = batch_size
    om0 = theta[:, 0]
    ol = theta[:, 1]
    w = theta[:, 2]
    ts = tf.shape(theta)
    zz = np.tile(z, (bs, 1))
    arg = tf.reshape(om0, [ts[0], 1])*(1 + z)**3 + tf.reshape((1 - om0 - ol), [ts[0], 1])*(1 + z)**2 + tf.reshape(ol, [ts[0], 1])*(1 + z)**(3*(1 + tf.reshape(w, [ts[0], 1])))
    EE = tf.sqrt(arg)
    return EE, arg


def modelo(theta, z):    
    om0 = theta[:, 0]
    ol = theta[:, 1]
    w = theta[:, 2]    
    omega_k = 1 - om0 - ol
    sig = tf.sign(omega_k)
    may = tf.reshape(1 + tf.sign(sig - 1), [batch_size, 1])
    men = tf.reshape(1 - tf.abs(sig), [batch_size, 1])
    eq = tf.reshape(1 - tf.sign(sig + 1), [batch_size, 1])    
    dl = may*f1(theta, z, omega_k) + eq*f3(theta, z, omega_k) + men*f2(theta, z, omega_k)
    # integral
    dist = 5*tf.log(dl + 1e-300)/np.log(10)
    return dist


def likelihood(mod, dat, sigma):
  """Log likelihood.
  mod: tensor (batch_size, ndata): model values
  dat: array (ndata): data values
  cov: array (ndata, ndata): data error
  """
  L = - 0.5*chi2(mod, dat, sigma)[0] + np.sum(-0.5*np.log(np.diag(2*np.pi*sigma**2)))
  return L


def chi2(mod, dat, cov):
  """
  mod: tensor (batch_size, ndata): model values
  dat: array (ndata): data values
  cov: array (ndata, ndata): data error
  """
  cov_p = np.diag(cov)
  AA = tf.reduce_sum(((dat - mod)/cov_p)**2, axis=1)
  BB = tf.reduce_sum((dat - mod)/cov_p**2, axis=1)
  CC = np.sum(1/cov_p**2)
  chi = AA - (BB**2)/CC
  return chi, BB/CC

class Potential:
  def __init__(self, dat, cov, z, prior):
    """Computes potential energy and its gradient.
    dat: array (ndata), data.
    sigma: array (ndata, ndata), data error
    z: array (ndata), redshift.
    prior: object, prior.
    """
    self.data = dat
    self.cov = cov
    self.z = z
    self.prior = prior

  def value(self, theta):
    """Returns potential value in a point or batch of points.
    theta: tensor (batch_size, ndim), point in parameter space.
    """
    mod = modelo(theta, self.z)
    self.u = - likelihood(mod, self.data, self.cov) - self.prior.get_log_pdf(theta) 
    return self.u

  def grad(self, theta):
    """Returns gradient value in a point or batch of points.
    theta: tensor (batch_size, ndim), point in parameter space.
    """
    self.value(theta)
    self.gradient = tf.gradients(self.u, theta)
    return self.gradient[0]

class Prior:
  def __init__(self, dist=None, low=None, high=None, mean=None, cov=None):
    """Defines some priors. Uniform and normal.
    dist: str, distribution name.
    low: array or list (ndim), uniform low limit.
    high: array or list (ndim), uniform high limit.
    mean: array or list (ndim), normal dist. mean.
    cov: array or list (ndim, ndim), normal dist. covariance.
    """
    self.dist = dist 
    if dist==None:
        self.u = tf.distributions.Uniform(low=low, high=high)
        vol = np.prod(np.abs((high - low)))
        self.pdf = tf.ones(batch_size)/vol
        self.logpdf = tf.log(self.pdf)
    elif dist=='normal':
        self.u = tf.contrib.distributions.MultivariateNormalFullCovariance(loc=mean, covariance_matrix=cov)
    
  def get_samples(self, n):
    """Get distribution samples.
    n: int, # of samples, batch size.
    """
    return tf.cast(self.u.sample(sample_shape=(n)), tf.float32)

  def get_pdf(self, value):
    """Get distribution pdf.
    value: tensor, point to be evaluated.
    """
    if self.dist==None:
      return self.pdf
    else:
      return self.u.prob(value)

  def get_log_pdf(self, value):
    """Get distribution log pdf.
    value: tensor, point to be evaluated.
    """
    if self.dist==None:
      return self.logpdf
    else:
      return self.u.log_prob(value)

"""#### CODE"""

class Leapfrog:
    """Leapfrog object, solve dynamics."""
    def __init__(self, U, m, ndim, e, b):
        """
        U: potential energy function
        m: # of leapfrog steps
        ndim: # of dimensions
        nnx: neural network of x's
        nnv: neural network of v's
        e: leapfrog step parameter
        b: batch size
        """
        self.U = U
        self.m = m
        self.ndim = ndim
        self.e = e
        self.b = b
        
       
    def for_dyn_fun(self):
        """One step of forward dynamics.
        """
        self.grad = self.U.grad(self.x)                       
        self.v = tf.squeeze(self.v - 0.5*self.e*self.grad)        
        self.x = tf.squeeze(self.x + self.e*self.v)        
        self.grad = self.U.grad(self.x)        
        self.v = tf.squeeze(self.v - 0.5*self.e*self.grad) 
        
        
    def dyn(self, x):
        """m steps dynamics"""
        self.resample()
        self.x = x
        for i in range(self.m):
           # update outputs of the neural network
            self.time_encoding(i) # updates time
            self.for_dyn_fun()            
        return self.x
     
    def get_value(self):
        return self.x
          
                
    def time_encoding(self, mi):
        """Encodes time.
        mi: int, actual leapfrog step.
        """
        arg = 2*np.pi*mi/self.m
        val = np.array([np.cos(arg), np.sin(arg)])
        val = np.tile(val, (self.b, 1))
        val = tf.convert_to_tensor(val)
        self.t = tf.cast(val, tf.float32)
        
        
    def resample(self):
        """Resamples velocity and direction"""
        self.v = tf.random_normal(shape=(self.b, self.ndim), mean=0.0, stddev=1.0, 
                                  dtype=tf.float32)

"""#### TESTING

#### CODE
"""

def lam(lamb, distance, acc):
  res = lamb**2/distance/acc
  return res - 1/res


def distance(x1, x2):
  return tf.reduce_sum((x1 - x2)**2, axis=1)

 
def acceptance(x1, x2, u):
    ax = tf.minimum(0., - u.value(x2) + u.value(x1)) 
    
    # 0 probablity for states out of the limits
    ax = limits(x2, ax)
    
    
    un = tf.log(tf.random_uniform(shape=(batch_size, 1), dtype=tf.float32))
    act = tf.maximum(0., tf.sign(un - tf.reshape(ax, (batch_size, 1))))
    nact = tf.ones_like(act) - act
    
    acc_prob = tf.exp(ax) + 1e-20
       
    acc_samples = nact*x2 + act*x1
    return acc_samples, acc_prob, tf.squeeze(nact)


def loss(l1, l2, lamb_b):
  return tf.reduce_mean(l1) + lamb_b*tf.reduce_mean(l2)


def Loss(p, p1, q, q1, u, lamb, lamb_b):
  # Calculates loss
  dp = distance(p, p1)
  dq = distance(q, q1)
  acp, pap, nactp = acceptance(p, p1, U) #LOSS.train(p, p1, q, q1) #
  acq, paq, nactq = acceptance(q, q1, U)
  lp = lam(lamb, dp, pap)
  lq = lam(lamb, dq, paq)
  return loss(lp, lq, lamb_b), acp, nactp


def limits(vec, vec2mask):
    mask1 = tf.cast(vec[:, 0]<=1.0, tf.float32)*tf.cast(vec[:, 0]>=0.0, tf.float32)
    mask2 = tf.cast(vec[:, 1]<=1.0, tf.float32)*tf.cast(vec[:, 1]>=0.0, tf.float32)
    mask3 = tf.cast(vec[:, 2]<=-1/3, tf.float32)*tf.cast(vec[:, 2]>=-4.0, tf.float32)
    #mask = tf.transpose(tf.stack([mask1, mask2, mask3]))
    mask = mask1*mask2*mask3 # tf.reduce_prod(mask, axis=1)
    nmask = tf.ones_like(mask) - mask
    #return vec2mask*mask + tf.log(mask)
    return vec2mask*mask - nmask*tf.ones_like(nmask)*1e20


def ratio(A):
    B = np.array(A)
    a1, _ = B.shape
    return np.sum(B, axis=0)/a1
  
  
def save(array, name):
    from google.colab import files
    # save training chains
    np.save(name, array)
    files.download('{}.npy'.format(name)) 
    return
  
  
def plot(arr1, arr2, arr3, names, save=False, savename=None):
    fig, ax = plt.subplots(nrows=2, ncols=2, sharex=False, sharey=False, figsize=(12, 12))
    ax1 = ax[0, 0]
    ax2 = ax[1, 0]
    ax3 = ax[1, 1]
    ax1.scatter(arr1, arr2, color='navy', marker='.', alpha=0.1)
    ax1.scatter(np.mean(arr1), np.mean(arr2), color='black', marker='+', label='expected value')
    ax1.scatter(arr1[0], arr2[0], color='black', label='initial state')
    ax2.scatter(arr1, arr3, color='red', marker='.', alpha=0.1)
    ax2.scatter(np.mean(arr1), np.mean(arr3), color='black', marker='+')
    ax2.scatter(arr1[0], arr3[0], color='black')
    ax3.scatter(arr2, arr3, color='green', marker='.', alpha=0.1)
    ax3.scatter(np.mean(arr2), np.mean(arr3), color='black', marker='+')
    ax3.scatter(arr2[0], arr3[0], color='black')
    ax1.set_title(savename)
    # labels
    ax2.set_xlabel(names[0])
    ax1.set_ylabel(names[1])
    ax2.set_ylabel(names[2])
    ax3.set_xlabel(names[1])
    # limits
    ax1.set_xlim([0, 1])
    ax1.set_ylim([0, 1])
    ax2.set_xlim([0, 1])
    ax2.set_ylim([-4, -1/3])
    ax3.set_xlim([0, 1])
    ax3.set_ylim([-4, -1/3])
    fig.legend()
    if save:
        fig.savefig(savename, dpi=dpi)
    return

# carga de datos

from google.colab import files
file = files.upload()
file = file['gal.txt'].split(b'\n')

redshift = []
mu_obs = []
cov = []
for i in file:
  try:
    j = i.split(b'\t')
    redshift.append(float(j[1]))
    mu_obs.append(float(j[2]))
    cov.append(float(j[3]))
  except:
    1
redshift = np.array(redshift)
mu_obs = np.array(mu_obs)
cov = np.array(cov)

p = np.argsort(redshift)
redshift = redshift[p].astype(np.float32)
mu_obs = mu_obs[p]
cov = cov[p]
cov = np.diag(cov)

"""## Chain"""

tf.reset_default_graph()

N = 10000 # chain steps
batch_size = 40
lfstep = 1e-2
lfsteps = 8

# some definitions
low = np.array([0., 0., - 4.])
high = np.array([1., 1., - 1/3])
prior_dist = Prior(dist=None, low=low, high=high)
U = Potential(dat=mu_obs, cov=cov, z=redshift, prior=prior_dist)
lpp = Leapfrog(U=U, m=lfsteps, ndim=3, e=lfstep, b=batch_size)

p = tf.placeholder(tf.float32, shape=(None, 3))
p_new = np.random.uniform(low=low, high=high, size=(batch_size, 3))
p_next = lpp.dyn(p)

new_sample = acceptance(p, p_next, U)

# tensorflow stuff
sess = tf.Session()
init = tf.global_variables_initializer()
sess.run(init)  

# list of states
points = []
accept = []
points.append(p_new)
accept.append(np.ones(batch_size))
ti = time.time()
for i in range(N):
  p_new, _, acce = sess.run(new_sample, {p: p_new}) 
  points.append(p_new)
  accept.append(acce)
  print('Chain step: {} Accept. ratio = {:.2f}'.format(i, np.median(ratio(accept))))
tfi = time.time()
print('Training time {:1f} min.'.format((tfi - ti)/60))

points = np.array(points)
accept = np.array(accept)

"""## Acceptance ratio plot"""

plt.figure(figsize=(18, 10))
plt.step(np.arange(N + 1), np.mean(accept, axis=1), color='navy')
plt.xlabel('chain step')
plt.ylabel('acceptance ratio')

"""## Chains save"""

np.save("chain_points_hmc", points)
files.download("chain_points_hmc.npy")