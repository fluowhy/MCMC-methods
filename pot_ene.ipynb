{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauricio/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate as sci\n",
    "import time\n",
    "from getdist import plots, MCSamples\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "ds = tf.contrib.distributions\n",
    "\n",
    "%matplotlib inline\n",
    "dpi = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved things directory\n",
    "direc = '/home/mauricio/Documents/Uni/Intro_2/' + 'gal.txt'\n",
    "\n",
    "# Carga de datos\n",
    "redshift = np.genfromtxt('gal.txt', usecols=(1))\n",
    "mu_obs = np.genfromtxt('gal.txt', usecols=(2)) # m - M\n",
    "cov = np.genfromtxt('gal.txt', usecols=(3))\n",
    "\n",
    "p = np.argsort(redshift)\n",
    "redshift = redshift[p]\n",
    "mu_obs = mu_obs[p]\n",
    "cov = cov[p]\n",
    "cov = np.diag(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def f1(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300))\n",
    "    o_k_s = tf.sqrt(abs(omk))\n",
    "    return (1 + z)*tf.sinh(o_k_s*I)/(o_k_s + 1e-300)\n",
    "\n",
    "\n",
    "def f2(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300))\n",
    "    o_k_s = tf.sqrt(abs(omk))\n",
    "    return (1 + z)*tf.sin(o_k_s*I)/(o_k_s + 1e-300)\n",
    "\n",
    "\n",
    "def f3(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300))\n",
    "    return (1 + z)*I\n",
    "\n",
    "\n",
    "def EHubble(theta, z): # parametro de hubble\n",
    "    om0 = theta[0]\n",
    "    ol = theta[1]\n",
    "    w = theta[2]\n",
    "    arg = om0*(1 + z)**3 + (1 - om0 - ol)*(1 + z)**2 + ol*(1 + z)**(3*(1 + w))\n",
    "    EE = tf.sqrt(arg)\n",
    "    return EE, arg\n",
    "\n",
    "\n",
    "def modelo(theta, z):\n",
    "    om0 = theta[0]\n",
    "    ol = theta[1]\n",
    "    w = theta[2]    \n",
    "    omega_k = 1 - om0 - ol\n",
    "    # integral    \n",
    "    dl = tf.case({tf.greater(omega_k, 0.): lambda: f1(theta, z, omega_k), \n",
    "                 tf.less(omega_k, 0.): lambda: f2(theta, z, omega_k)}, \n",
    "                default=lambda: f3(theta, z, omega_k), exclusive=True)\n",
    "    dist = 5*tf.log(dl + 1e-300)/np.log(10)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class potential:\n",
    "    def __init__(self, dat, sigma, z):\n",
    "        self.data = dat\n",
    "        self.cov = sigma\n",
    "        self.z = z\n",
    "    \n",
    "    def value(self, theta):\n",
    "        self.mod = modelo(theta, self.z)\n",
    "        self.u = - likelihood(self.mod, self.data, self.cov) #- prior(theta, ndim) \n",
    "        return self.u\n",
    "    \n",
    "    def grad(self, theta):\n",
    "        self.gradient = tf.gradients(self.u, theta)\n",
    "        return self.gradient\n",
    "\n",
    "\n",
    "def likelihood(mod, dat, sigma): # retorna escalar, log(L)\n",
    "    \"\"\"Log likelihood\n",
    "    mod: tf.tensor with model results\n",
    "    dat: numpy array of data\n",
    "    sigma: numpy array of covariance\"\"\"\n",
    "    sig = tf.cast(tf.diag_part(sigma), tf.float32)\n",
    "    L = -0.5*chi2(mod, dat, sigma)[0]  + tf.reduce_sum(-0.5*tf.log(2*np.pi*sig**2))\n",
    "    return L\n",
    "\n",
    "\n",
    "def chi2(mod, dat, sigma):\n",
    "    sig = tf.cast(tf.diag_part(sigma), tf.float32)\n",
    "    AA = tf.reduce_sum(tf.square((dat - mod)/sig))\n",
    "    BB = tf.reduce_sum((dat - mod)/tf.square(sig))\n",
    "    CC = tf.reduce_sum(1/tf.square(sig))\n",
    "    chi = AA - (BB**2)/CC\n",
    "    return chi, BB/CC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior:\n",
    "    def __init__(self, name, low=None, high=None, mean=None, cov=None):\n",
    "        if name=='uniform':\n",
    "            self.u = tf.distributions.Uniform(low=low, high=high)\n",
    "        elif name=='normal':\n",
    "            self.u = tf.contrib.distributions.MultivariateNormalFullCovariance(loc=mean, covariance_matrix=cov)\n",
    "    \n",
    "    def get_samples(self, n):\n",
    "        return self.u.sample(sample_shape=(n))\n",
    "    \n",
    "    def get_pdf(self, value):\n",
    "        return self.u.prob(value)\n",
    "    \n",
    "    def get_log_pdf(self, value):\n",
    "        return self.u.log_prob(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_encodig(m, m_act):\n",
    "    arg = 2*np.pi*m_act/m\n",
    "    val = np.array([np.cos(arg), np.sin(arg)])\n",
    "    return tf.assign(t1, val)\n",
    "\n",
    "\n",
    "class Leapfrog:\n",
    "    def __init__(self, U, m, ndim, nnx, nnv, e, b):\n",
    "        \"\"\"\n",
    "        U: potential energy function\n",
    "        m: # of leapfrog steps\n",
    "        ndim: # of dimensions\n",
    "        nnx: neural network of x's\n",
    "        nnv: neural network of v's\n",
    "        e: leapfrog step parameter\n",
    "        b: batch size\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.m = m\n",
    "        self.ndim = ndim\n",
    "        self.nnx = nnx\n",
    "        self.nnv = nnv\n",
    "        self.e = e\n",
    "        self.b = b\n",
    "        \n",
    "    def direction(self):\n",
    "        self.d = np.random.choice([-1, 1])\n",
    "        \n",
    "    def for_dyn(self, x0, v0):\n",
    "        \"\"\"One step of forward dynamics d=1.\n",
    "        x0: actual position in parameter space.\n",
    "        v0: actual velocity.\n",
    "        \"\"\"\n",
    "        # remember S, Q, T update in each sub iteration\n",
    "        self.v = v0*tf.exp(0.5*self.nnv.S*self.e) - 0.5*self.e*(self.U.grad(x0)*tf.exp(self.nnv.Q*self.e) \n",
    "                                                                + self.nnv.T)\n",
    "        self.x = x0*tf.exp(self.e*self.nnx.S) + self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)\n",
    "        self.v = self.v*tf.exp(0.5*self.nnv.S*self.e) - 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) \n",
    "                                                                    + self.nnv.T)\n",
    "    \n",
    "    def back_dyn(self, x0, v0):\n",
    "        \"\"\"One step of backward dynamics d=-1\n",
    "        x0: actual position in parameter space.\n",
    "        v0: actual velocity.\n",
    "        \"\"\"\n",
    "        self.v = v0*tf.exp(- 0.5*self.nnv.S*self.e) + 0.5*self.e*(self.U.grad(x0)*tf.exp(self.nnv.Q*self.e) \n",
    "                                                                + self.nnv.T)\n",
    "        self.x = x0*tf.exp(- self.e*self.nnx.S) - self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)\n",
    "        self.v = self.v*tf.exp(- 0.5*self.nnv.S*self.e) + 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) \n",
    "                                                                    + self.nnv.T)\n",
    "        \n",
    "    def dyn(self, x0, v0):\n",
    "        \"\"\"Dynamics.\n",
    "        x0: inital position in parameter space\n",
    "        v0: inital velocity.\n",
    "        \"\"\"\n",
    "        self.direction()\n",
    "        for i in range(self.m):\n",
    "            #self.mask = np.random.choice([1, 0], size=(self.b, self.ndim), p=[0.5, 0.5])\n",
    "            #self.nmask = np.ones((self.b, self.ndim)) - self.mask\n",
    "            if self.d==1:\n",
    "                self.for_dyn(x0, v0)\n",
    "            elif self.d==-1:\n",
    "                self.back_dyn(x0, v0)\n",
    "            x0 = self.x\n",
    "            v0 = self.v\n",
    "        return self.x, self.v\n",
    "        \n",
    "    def resample(self):\n",
    "        self.v = tf.random_normal(shape=(self.b, self.ndim), mean=0.0, stddev=1.0, dtype=tf.float32)\n",
    "        self.direction()\n",
    "    \n",
    "    def flip(self):\n",
    "        self.d *= -1\n",
    "    \n",
    "        \n",
    "class MLP:\n",
    "    def __init__(self, ndim, n1, n2, ls, lq):\n",
    "        \"\"\"\n",
    "        ndim: # of dimensions\n",
    "        n1: # of neurons of layer 1\n",
    "        ls: output parameter\n",
    "        lq: output parameter\n",
    "        \"\"\"\n",
    "        self.W1 = tf.Variable(tf.random_normal([ndim, n1]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([ndim, n1]))\n",
    "        self.W3 = tf.Variable(tf.random_normal([2, n1])) # time encoding\n",
    "        self.W4 = tf.Variable(tf.random_normal([n1, n2]))\n",
    "        self.Ws = tf.Variable(tf.random_normal([n2, 1]))\n",
    "        self.Wq = tf.Variable(tf.random_normal([n2, 1]))\n",
    "        self.Wt = tf.Variable(tf.random_normal([n1, 1]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([n1]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([n2]))\n",
    "        self.bs = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.bq = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.bt = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.ls = ls\n",
    "        self.lq = lq\n",
    "        \n",
    "    \n",
    "    def model(self, x, v, t):\n",
    "        h1 = tf.matmul(x, self.W1) + tf.matmul(v, self.W2) + tf.matmul(t, self.W3) + self.b1\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h2 = tf.matmul(h1, self.W4) + self.b2\n",
    "        h2 = tf.nn.relu(h2)\n",
    "        self.S = tf.tanh(tf.matmul(h2, self.Ws) + self.bs)\n",
    "        self.Q = tf.tanh(tf.matmul(h2, self.Wq) + self.bq)\n",
    "        self.T = tf.matmul(h2, self.Wt) + self.bt\n",
    "    \n",
    "    \n",
    "    #loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=Y))\n",
    "    #optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "    #train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "class L2HMC:\n",
    "    def __init__(self, U, prior, n, b, m, lr, sc, reg):\n",
    "        \"\"\"\n",
    "        U: energy function\n",
    "        prior: prior distribution\n",
    "        n: # iterations\n",
    "        b: batch size\n",
    "        m: leapfrog steps\n",
    "        lr: learning rate\n",
    "        sc: scale parameter\n",
    "        reg: regularization parameter\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.prior = prior\n",
    "        self.n = n\n",
    "        self.b = b\n",
    "        self.m = m\n",
    "        self.lr = lr\n",
    "        self.sc = sc\n",
    "        self.reg = reg\n",
    "        self.init_samples = self.prior.get_samples(b)\n",
    "        \n",
    "    def run(self):\n",
    "        for i in range(self.n):\n",
    "            #print(i)\n",
    "            self.mini_batch = self.prior.get_samples(self.b)\n",
    "            for j in range(self.b):\n",
    "                pass\n",
    "                #print(j)\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = [0., 0., -6.]\n",
    "high = [1., 1., 1/3]\n",
    "batch_size = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.5  0.4 -0.5]\n",
      " [ 0.5  0.4 -0.5]]\n",
      "[1. 2. 3.]\n",
      "[[ 0.5  0.8 -1.5]\n",
      " [ 0.5  0.8 -1.5]]\n"
     ]
    }
   ],
   "source": [
    "pot = potential(mu_obs, cov, redshift)\n",
    "pr = prior(name='uniform', low=low, high=high)\n",
    "l2 = L2HMC(pot, pr, 10, batch_size, 2, 1e-3, 1e-1, 1e-1)\n",
    "mlpx = MLP(ndim=3, n1=10, n2=10, ls=1, lq=1)\n",
    "mlpv = MLP(ndim=3, n1=10, n2=10, ls=1, lq=1)\n",
    "\n",
    "\n",
    "x1 = np.array([[0.5, 0.4, -0.5], [0.5, 0.4, -0.5]])\n",
    "x1 = tf.convert_to_tensor(x1, dtype=tf.float32)\n",
    "\n",
    "v1 = np.array([[0.3, 0.4, -1.5], [0.3, 0.4, -1.5]])\n",
    "v1 = tf.convert_to_tensor(v1, dtype=tf.float32)\n",
    "\n",
    "t1 = np.zeros((2, 2))\n",
    "t1 = tf.convert_to_tensor(t1, dtype=tf.float32)\n",
    "\n",
    "a1 = np.array([1, 2, 3])\n",
    "a1 = tf.convert_to_tensor(a1, dtype=tf.float32)\n",
    "\n",
    "mlpx.model(x1, v1, t1)\n",
    "\n",
    "t2 = x1*a1\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(x1))\n",
    "    print(sess.run(a1))\n",
    "    print(sess.run(t2))\n",
    "l2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1\n",
      "Tensor(\"random_normal_169:0\", shape=(5, 3), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "lp = Leapfrog(U=pot, m=5, ndim=3, nnx=mlpx, nnv=mlpv, e=1e-3, b=batch_size)\n",
    "lp.resample()\n",
    "print(lp.d)\n",
    "print(lp.v)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
