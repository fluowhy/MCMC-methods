{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mauricio/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import integrate as sci\n",
    "import time\n",
    "from getdist import plots, MCSamples\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "ds = tf.contrib.distributions\n",
    "\n",
    "%matplotlib inline\n",
    "dpi = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saved things directory\n",
    "direc = '/home/mauricio/Documents/Uni/Intro_2/' + 'gal.txt'\n",
    "\n",
    "# Carga de datos\n",
    "redshift = np.genfromtxt('gal.txt', usecols=(1))\n",
    "mu_obs = np.genfromtxt('gal.txt', usecols=(2)) # m - M\n",
    "cov = np.genfromtxt('gal.txt', usecols=(3))\n",
    "\n",
    "p = np.argsort(redshift)\n",
    "redshift = redshift[p].astype(np.float32)\n",
    "mu_obs = mu_obs[p]\n",
    "cov = cov[p]\n",
    "cov = np.diag(cov)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "def f1(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300), axis=1)\n",
    "    o_k_s = tf.reshape(tf.sqrt(abs(omk)), [batch_size, 1])\n",
    "    return (1 + z)*tf.sinh(o_k_s*I)/(o_k_s + 1e-300)\n",
    "\n",
    "\n",
    "def f2(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300), axis=1)\n",
    "    o_k_s = tf.reshape(tf.sqrt(abs(omk)), [batch_size, 1])\n",
    "    return (1 + z)*tf.sin(o_k_s*I)/(o_k_s + 1e-300)\n",
    "\n",
    "\n",
    "def f3(theta, z, omk):\n",
    "    zc = np.copy(z)\n",
    "    zc = np.insert(zc, 0, 0)\n",
    "    dz = zc[1:] - zc[:-1]\n",
    "    E = EHubble(theta, z)[0]\n",
    "    I = tf.cumsum(dz/(E + 1e-300), axis=1)\n",
    "    return (1 + z)*I\n",
    "\n",
    "\n",
    "def EHubble(theta, z): # parametro de hubble\n",
    "    \"\"\"\n",
    "    theta: parameter space state.\n",
    "    z: redshift.\n",
    "    bs: batch size.\n",
    "    \"\"\"\n",
    "    bs = batch_size\n",
    "    om0 = theta[:, 0]\n",
    "    ol = theta[:, 1]\n",
    "    w = theta[:, 2]\n",
    "    ts = tf.shape(theta)\n",
    "    zz = np.tile(z, (bs, 1))\n",
    "    arg = tf.reshape(om0, [ts[0], 1])*(1 + z)**3 + tf.reshape((1 - om0 - ol), [ts[0], 1])*(1 + z)**2 + tf.reshape(ol, [ts[0], 1])*(1 + z)**(3*(1 + tf.reshape(w, [ts[0], 1])))\n",
    "    EE = tf.sqrt(arg)\n",
    "    return EE, arg\n",
    "\n",
    "\n",
    "def modelo(theta, z):    \n",
    "    om0 = theta[:, 0]\n",
    "    ol = theta[:, 1]\n",
    "    w = theta[:, 2]    \n",
    "    omega_k = 1 - om0 - ol\n",
    "    sig = tf.sign(omega_k)\n",
    "    may = tf.reshape(1 + tf.sign(sig - 1), [batch_size, 1])\n",
    "    men = tf.reshape(1 - tf.abs(sig), [batch_size, 1])\n",
    "    eq = tf.reshape(1 - tf.sign(sig + 1), [batch_size, 1])    \n",
    "    dl = may*f1(theta, z, omega_k) + eq*f3(theta, z, omega_k) + men*f2(theta, z, omega_k)\n",
    "    # integral\n",
    "    dist = 5*tf.log(dl + 1e-300)/np.log(10)\n",
    "    return dist\n",
    "\n",
    "\n",
    "class potential:\n",
    "    def __init__(self, dat, sigma, z):\n",
    "        self.data = dat\n",
    "        self.cov = sigma\n",
    "        self.z = z\n",
    "    \n",
    "    def value(self, theta):\n",
    "        self.mod = modelo(theta, self.z)\n",
    "        self.u = - likelihood(self.mod, self.data, self.cov) #- prior(theta, ndim) \n",
    "        return self.u\n",
    "    \n",
    "    def grad(self, theta):\n",
    "        self.mod = modelo(theta, self.z)\n",
    "        self.u = - likelihood(self.mod, self.data, self.cov)\n",
    "        self.gradient = tf.gradients(self.u, theta)\n",
    "        return self.gradient\n",
    "\n",
    "\n",
    "def likelihood(mod, dat, sigma): # retorna escalar, log(L)\n",
    "    \"\"\"Log likelihood\n",
    "    mod: tf.tensor with model results\n",
    "    dat: numpy array of data\n",
    "    sigma: numpy array of covariance\"\"\"\n",
    "    sig = tf.cast(tf.diag_part(sigma), tf.float32)\n",
    "    L = -0.5*chi2(mod, dat, sigma)[0]  + tf.reduce_sum(-0.5*tf.log(2*np.pi*sig**2))\n",
    "    return L\n",
    "\n",
    "\n",
    "def chi2(mod, dat, sigma):\n",
    "    dat1 = np.tile(dat, (batch_size, 1))\n",
    "    sig = tf.cast(tf.diag_part(sigma), tf.float32)\n",
    "    sig1 = np.tile(sig, (batch_size, 1))\n",
    "    AA = tf.reduce_sum(tf.square((dat1 - mod)/sig))\n",
    "    BB = tf.reduce_sum((dat1 - mod)/tf.square(sig))\n",
    "    CC = tf.reduce_sum(1/tf.square(sig))\n",
    "    chi = AA - (BB**2)/CC\n",
    "    return chi, BB/CC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior:\n",
    "    def __init__(self, name, low=None, high=None, mean=None, cov=None):\n",
    "        if name=='uniform':\n",
    "            self.u = tf.distributions.Uniform(low=low, high=high)\n",
    "        elif name=='normal':\n",
    "            self.u = tf.contrib.distributions.MultivariateNormalFullCovariance(loc=mean, covariance_matrix=cov)\n",
    "    \n",
    "    def get_samples(self, n):\n",
    "        return self.u.sample(sample_shape=(n))\n",
    "    \n",
    "    def get_pdf(self, value):\n",
    "        return self.u.prob(value)\n",
    "    \n",
    "    def get_log_pdf(self, value):\n",
    "        return self.u.log_prob(value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Leapfrog:\n",
    "    def __init__(self, U, m, ndim, nnx, nnv, e, b):\n",
    "        \"\"\"\n",
    "        U: potential energy function\n",
    "        m: # of leapfrog steps\n",
    "        ndim: # of dimensions\n",
    "        nnx: neural network of x's\n",
    "        nnv: neural network of v's\n",
    "        e: leapfrog step parameter\n",
    "        b: batch size\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.m = m\n",
    "        self.ndim = ndim\n",
    "        self.nnx = nnx\n",
    "        self.nnv = nnv\n",
    "        self.e = e\n",
    "        self.b = b\n",
    "        self.x = tf.Variable(tf.random_normal([b, ndim]))\n",
    "        self.v = tf.Variable(tf.random_normal([b, ndim]))\n",
    "        self.t = tf.Variable(tf.random_normal([b, 2]))\n",
    "        \n",
    "    def direction(self):\n",
    "        \"\"\"Samples a new direction\"\"\"\n",
    "        self.d = np.random.choice([-1, 1])\n",
    "        \n",
    "    def for_dyn(self):\n",
    "        \"\"\"One step of forward dynamics d=1.\n",
    "        \"\"\"\n",
    "        # remember S, Q, T update in each sub iteration\n",
    "        \"\"\"\n",
    "        self.v = self.v*tf.exp(0.5*self.nnv.S*self.e) - 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        self.x = self.x*tf.exp(self.e*self.nnx.S) + self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)\n",
    "        self.x = tf.squeeze(self.x)\n",
    "        self.v = tf.squeeze(self.v)        \n",
    "        self.v = self.v*tf.exp(0.5*self.nnv.S*self.e) - 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        \"\"\"\n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(0.5*self.nnv.S*self.e) - \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "        tf.assign(self.x, \n",
    "                  tf.squeeze(self.x*tf.exp(self.e*self.nnx.S) + \n",
    "                             self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)))      \n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(0.5*self.nnv.S*self.e) - \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "    \n",
    "    def back_dyn(self):\n",
    "        \"\"\"One step of backward dynamics d=-1.\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.v = self.v*tf.exp(- 0.5*self.nnv.S*self.e) + 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        self.x = self.x*tf.exp(- self.e*self.nnx.S) - self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)\n",
    "        self.x = tf.squeeze(self.x)\n",
    "        self.v = tf.squeeze(self.v) \n",
    "        self.v = self.v*tf.exp(- 0.5*self.nnv.S*self.e) + 0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + self.nnv.T)\n",
    "        \"\"\"\n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(- 0.5*self.nnv.S*self.e) + \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "        tf.assign(self.x, \n",
    "                  tf.squeeze(self.x*tf.exp(- self.e*self.nnx.S) - self.e*(self.v*tf.exp(self.e*self.nnx.Q) + self.nnx.T)))\n",
    "        tf.assign(self.v, \n",
    "                  tf.squeeze(self.v*tf.exp(- 0.5*self.nnv.S*self.e) + \n",
    "                             0.5*self.e*(self.U.grad(self.x)*tf.exp(self.nnv.Q*self.e) + \n",
    "                                         self.nnv.T)))\n",
    "        \n",
    "    def dyn(self):\n",
    "        \"\"\"m steps dynamics\"\"\"\n",
    "        #self.direction()\n",
    "        for i in range(self.m):\n",
    "            #self.mask = np.random.choice([1, 0], size=(self.b, self.ndim), p=[0.5, 0.5])\n",
    "            #self.nmask = np.ones((self.b, self.ndim)) - self.mask\n",
    "            # update outputs of the neural network\n",
    "            self.time_encoding(i) # updates time\n",
    "            self.nnx.model(self.x, self.v, self.t)\n",
    "            self.nnv.model(self.x, self.v, self.t)\n",
    "            if self.d==1:\n",
    "                self.for_dyn()\n",
    "            elif self.d==-1:\n",
    "                self.back_dyn()\n",
    "                \n",
    "                \n",
    "    def time_encoding(self, mi):\n",
    "        \"\"\"Encodes time.\n",
    "        mi: int, actual leapfrog step.\n",
    "        \"\"\"\n",
    "        arg = 2*np.pi*mi/self.m\n",
    "        val = np.array([np.cos(arg), np.sin(arg)])\n",
    "        val = np.tile(val, (self.b, 1))\n",
    "        val = tf.convert_to_tensor(val)\n",
    "        tf.assign(self.t, tf.cast(val, tf.float32))\n",
    "        \n",
    "        \n",
    "    def resample(self):\n",
    "        \"\"\"Resamples velocity and direction\"\"\"\n",
    "        tf.assign(self.v, tf.random_normal(shape=(self.b, self.ndim), \n",
    "                                           mean=0.0, stddev=1.0, dtype=tf.float32))\n",
    "        self.direction()\n",
    "        \n",
    "        \n",
    "    def flip(self):\n",
    "        \"\"\"Flip direction\"\"\"\n",
    "        self.d *= -1      \n",
    "    \n",
    "    def sampling(self):\n",
    "        \"\"\"Sampling operation\"\"\"\n",
    "        self.direction()\n",
    "        self.dyn()\n",
    "        self.flip()\n",
    "    \n",
    "    def update_state(self, x0):\n",
    "        tf.assign(self.x, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2HMC:\n",
    "    def __init__(self, U, prior, n, b, m, lr, sc, reg, lfp, lfq, x0):\n",
    "        \"\"\"\n",
    "        U: energy function\n",
    "        prior: prior distribution\n",
    "        n: # iterations\n",
    "        b: batch size\n",
    "        m: leapfrog steps\n",
    "        lr: learning rate\n",
    "        sc: scale parameter\n",
    "        reg: regularization parameter\n",
    "        lfp: inital samples leapfrog object\n",
    "        lfq: batch samples leapfrog object\n",
    "        x0: initial position in parameter space\n",
    "        \"\"\"\n",
    "        self.U = U\n",
    "        self.prior = prior\n",
    "        self.n = n\n",
    "        self.b = b\n",
    "        self.m = m\n",
    "        self.lr = lr\n",
    "        self.sc = sc\n",
    "        self.reg = reg\n",
    "        self.init_samples = self.prior.get_samples(b)\n",
    "        self.lfp = lfp\n",
    "        self.lfq = lfq\n",
    "        self.X = x0 # ep\n",
    "        self.optimizer = tf.train.AdamOptimizer(lr)\n",
    "    \n",
    "        \n",
    "    def distance(self):\n",
    "        self.dp = tf.reduce_sum((self.X - self.X1)**2, axis=1)\n",
    "        self.dq = tf.reduce_sum((self.Xq - self.xq)**2, axis=1)\n",
    "    \n",
    "    def acceptance(self):\n",
    "        pass\n",
    "        \n",
    "    def loss(self, d): \n",
    "        return self.sc**2/d - d/self.sc**2\n",
    "        \n",
    "    def Loss(self):\n",
    "        self.distance()\n",
    "        self.acceptance()\n",
    "        self.L0ss = (tf.reduce_mean(self.loss(self.dp)) \n",
    "                     + tf.reduce_mean(self.reg*self.loss(self.dq)))\n",
    "        \n",
    "    def train(self):\n",
    "        self.Loss()\n",
    "        self.optimizer.minimize(self.L0ss)\n",
    "        \n",
    "    def Run(self):\n",
    "        for i in range(self.n):\n",
    "            print(i)\n",
    "            self.xq = self.prior.get_samples(self.b) # from prior distribution eq\n",
    "            # first updates samples state \n",
    "            self.lfp.update_state(self.X) # now leapfrog knows the state to evolve\n",
    "            self.lfp.resample()\n",
    "            self.lfp.sampling()\n",
    "            self.X1 = self.lfp.x\n",
    "            # now updates batch samples state\n",
    "            self.lfq.update_state(self.xq) # now leapfrog knows the state to evolve\n",
    "            self.lfq.resample()\n",
    "            self.lfq.sampling()\n",
    "            self.Xq = self.lfq.x\n",
    "            # acceptance\n",
    "            self.train()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, ndim, n1, n2, ls, lq):\n",
    "        \"\"\"\n",
    "        ndim: # of dimensions\n",
    "        n1: # of neurons of layer 1\n",
    "        ls: output parameter\n",
    "        lq: output parameter\n",
    "        \"\"\"\n",
    "        self.W1 = tf.Variable(tf.random_normal([ndim, n1]))\n",
    "        self.W2 = tf.Variable(tf.random_normal([ndim, n1]))\n",
    "        self.W3 = tf.Variable(tf.random_normal([2, n1])) # time encoding\n",
    "        self.W4 = tf.Variable(tf.random_normal([n1, n2]))\n",
    "        self.Ws = tf.Variable(tf.random_normal([n2, 1]))\n",
    "        self.Wq = tf.Variable(tf.random_normal([n2, 1]))\n",
    "        self.Wt = tf.Variable(tf.random_normal([n1, 1]))\n",
    "        self.b1 = tf.Variable(tf.random_normal([n1]))\n",
    "        self.b2 = tf.Variable(tf.random_normal([n2]))\n",
    "        self.bs = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.bq = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.bt = tf.Variable(tf.random_normal([ndim]))\n",
    "        self.ls = ls\n",
    "        self.lq = lq\n",
    "    \n",
    "    def model(self, x, v, t):\n",
    "        h1 = tf.matmul(x, self.W1) + tf.matmul(v, self.W2) + tf.matmul(t, self.W3) + self.b1\n",
    "        h1 = tf.nn.relu(h1)\n",
    "        h2 = tf.matmul(h1, self.W4) + self.b2\n",
    "        h2 = tf.nn.relu(h2)\n",
    "        self.S = tf.tanh(tf.matmul(h2, self.Ws) + self.bs)\n",
    "        self.Q = tf.tanh(tf.matmul(h2, self.Wq) + self.bq)\n",
    "        self.T = tf.matmul(h2, self.Wt) + self.bt\n",
    "    \n",
    "    \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "low = [0., 0., -6.]\n",
    "high = [1., 1., 1/3]\n",
    "global batch_size\n",
    "batch_size = 2\n",
    "\n",
    "x1 = np.array([[0.5, 0.4, -0.5], [0.5, 0.4, -0.5]])\n",
    "x1 = tf.convert_to_tensor(x1, dtype=tf.float32)\n",
    "\n",
    "v1 = np.array([[0.3, 0.4, -1.5], [0.3, 0.4, -1.5]])\n",
    "v1 = tf.convert_to_tensor(v1, dtype=tf.float32)\n",
    "\n",
    "t1 = np.zeros((2, 2))\n",
    "t1 = tf.convert_to_tensor(t1, dtype=tf.float32)\n",
    "\n",
    "a1 = np.array([1, 2, 3])\n",
    "a1 = tf.convert_to_tensor(a1, dtype=tf.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pot = potential(mu_obs, cov, redshift)\n",
    "pr = prior(name='uniform', low=low, high=high)\n",
    "mlpx = MLP(ndim=3, n1=10, n2=10, ls=1, lq=1)\n",
    "mlpv = MLP(ndim=3, n1=10, n2=10, ls=1, lq=1)\n",
    "mlpx.model(x1, v1, t1)\n",
    "mlpv.model(x1, v1, t1)\n",
    "lfp = Leapfrog(U=pot, m=2, ndim=3, nnx=mlpx, nnv=mlpv, e=1e-3, b=batch_size)\n",
    "lfq = Leapfrog(U=pot, m=2, ndim=3, nnx=mlpx, nnv=mlpv, e=1e-3, b=batch_size)\n",
    "l2 = L2HMC(pot, pr, 2, batch_size, 2, 1e-3, 1e-1, 1e-1, lfp, lfq, x1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "[[-0.1123492  -0.5204956  -0.16787426 -0.9697638  -0.5113813  -0.4466553\n",
      "   0.6632534  -0.20490135  1.4734832   0.447116  ]\n",
      " [-0.27790308  0.12932882 -1.6955649  -1.3449037   0.6442632   1.5062255\n",
      "  -1.7222717  -0.61809576  3.204022   -0.6150305 ]\n",
      " [ 0.58014315 -0.23208265  0.6887078   0.08456226 -2.0291688  -0.06854253\n",
      "  -0.29231626 -1.8247484   0.740344    1.8968562 ]]\n",
      "[[-0.8560113  -0.6002867  -1.1473316   0.9116065  -0.17145513 -0.29820508\n",
      "  -0.5367889   0.74173576 -1.762843    2.9269288 ]\n",
      " [-0.5501381   0.24169864 -0.37297893  1.480622    0.67826414 -0.1811315\n",
      "   0.24152836 -0.05104424 -0.22892423 -1.9216114 ]\n",
      " [ 0.62063205 -0.28846878  2.134821   -0.0208789   0.31769362  0.4780832\n",
      "   0.15362573 -0.5068271   0.20517947  1.6152414 ]]\n"
     ]
    }
   ],
   "source": [
    "l2.Run()\n",
    "init = tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    print(sess.run(l2.lfp.nnx.W1))\n",
    "    print(sess.run(l2.lfp.nnv.W1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.015     , 0.015     , 0.015     , 0.01502704, 0.0151    ,\n",
       "       0.015166  , 0.0152    , 0.0152    , 0.0153    , 0.0154363 ,\n",
       "       0.016     , 0.016     , 0.0163    , 0.016321  , 0.01634564,\n",
       "       0.01645   , 0.016559  , 0.01673   , 0.016743  , 0.016991  ,\n",
       "       0.017173  , 0.017227  , 0.0173    , 0.0173    , 0.017605  ,\n",
       "       0.01793128, 0.01831523, 0.0187    , 0.0189    , 0.0192    ,\n",
       "       0.0192    , 0.019264  , 0.0195    , 0.019599  , 0.02037472,\n",
       "       0.0205    , 0.0208    , 0.0209    , 0.0211    , 0.0212    ,\n",
       "       0.0215    , 0.021793  , 0.0219    , 0.02198001, 0.0221    ,\n",
       "       0.0221    , 0.0224    , 0.0229    , 0.0229    , 0.02297117,\n",
       "       0.023     , 0.023208  , 0.0233    , 0.0233    , 0.023536  ,\n",
       "       0.0239    , 0.023953  , 0.024     , 0.0241853 , 0.0242    ,\n",
       "       0.024314  , 0.024525  , 0.0247    , 0.0248    , 0.0249    ,\n",
       "       0.0251    , 0.0255    , 0.0256    , 0.0259    , 0.026     ,\n",
       "       0.026038  , 0.0261    , 0.026489  , 0.0266    , 0.0268092 ,\n",
       "       0.027342  , 0.0275    , 0.02756873, 0.0277    , 0.027865  ,\n",
       "       0.0283    , 0.02839603, 0.028488  , 0.0292    , 0.02980214,\n",
       "       0.029955  , 0.0303    , 0.030529  , 0.030604  , 0.0308    ,\n",
       "       0.0308    , 0.0308    , 0.0309    , 0.0312    , 0.0312    ,\n",
       "       0.0315    , 0.031528  , 0.032     , 0.032     , 0.0321    ,\n",
       "       0.0321    , 0.0321    , 0.03213402, 0.0325    , 0.0327    ,\n",
       "       0.0329    , 0.03291237, 0.0334    , 0.0335    , 0.0336    ,\n",
       "       0.0337    , 0.0341    , 0.0341    , 0.0341    , 0.0345    ,\n",
       "       0.03572   , 0.036     , 0.036     , 0.036     , 0.0362    ,\n",
       "       0.036457  , 0.03648   , 0.0366    , 0.0377    , 0.0393    ,\n",
       "       0.0402    , 0.0406    , 0.0421    , 0.042233  , 0.0423    ,\n",
       "       0.0425    , 0.04371891, 0.04497667, 0.045295  , 0.04696734,\n",
       "       0.0483922 , 0.048818  , 0.048948  , 0.0491    , 0.049922  ,\n",
       "       0.050043  , 0.0522    , 0.052926  , 0.05371   , 0.0544    ,\n",
       "       0.0546    , 0.05668337, 0.0576    , 0.0583    , 0.0583    ,\n",
       "       0.0589    , 0.06183577, 0.062668  , 0.06386408, 0.0643    ,\n",
       "       0.0651    , 0.06644031, 0.0684    , 0.0688    , 0.069     ,\n",
       "       0.070086  , 0.074605  , 0.07535011, 0.0784    , 0.078577  ,\n",
       "       0.08004814, 0.0843    , 0.08568946, 0.08569612, 0.08585464,\n",
       "       0.087589  , 0.08901943, 0.09293682, 0.0931494 , 0.09390863,\n",
       "       0.100915  , 0.10271503, 0.10671234, 0.10863826, 0.11304265,\n",
       "       0.11471262, 0.11634851, 0.11727736, 0.11762533, 0.11967154,\n",
       "       0.1228289 , 0.1241    , 0.12427353, 0.1244    , 0.12647316,\n",
       "       0.12668799, 0.12872674, 0.12927821, 0.1299    , 0.141788  ,\n",
       "       0.14240465, 0.1437059 , 0.1441    , 0.14462109, 0.14566855,\n",
       "       0.1462903 , 0.14702514, 0.1518579 , 0.15463209, 0.15524733,\n",
       "       0.1561    , 0.159     , 0.15988994, 0.16086185, 0.16379589,\n",
       "       0.1706284 , 0.172     , 0.17274223, 0.17391005, 0.1776007 ,\n",
       "       0.178     , 0.17968564, 0.18      , 0.18011978, 0.181     ,\n",
       "       0.18221824, 0.18254891, 0.1835684 , 0.18581244, 0.186     ,\n",
       "       0.18885317, 0.18970656, 0.19214998, 0.1943165 , 0.19671607,\n",
       "       0.20061173, 0.20260867, 0.20497969, 0.205     , 0.2109384 ,\n",
       "       0.21158698, 0.2116296 , 0.21254876, 0.213     , 0.21456826,\n",
       "       0.215     , 0.21554331, 0.216     , 0.216     , 0.21658282,\n",
       "       0.218     , 0.21834745, 0.2185852 , 0.22852847, 0.23278111,\n",
       "       0.239     , 0.24      , 0.24250469, 0.24437888, 0.24850813,\n",
       "       0.2486    , 0.24951105, 0.25066763, 0.2517402 , 0.25248605,\n",
       "       0.25549063, 0.25647575, 0.25749788, 0.2577403 , 0.25802827,\n",
       "       0.26      , 0.26053348, 0.2605861 , 0.263     , 0.26349103,\n",
       "       0.26364794, 0.26576248, 0.266     , 0.269     , 0.27043444,\n",
       "       0.271     , 0.27345476, 0.274     , 0.2754402 , 0.27785343,\n",
       "       0.278     , 0.27892467, 0.27945474, 0.284     , 0.285     ,\n",
       "       0.286     , 0.2866187 , 0.28841835, 0.2912    , 0.29246977,\n",
       "       0.29558554, 0.29751882, 0.29840928, 0.29877743, 0.3       ,\n",
       "       0.3003127 , 0.30175504, 0.302     , 0.30240163, 0.30858088,\n",
       "       0.309     , 0.309     , 0.30949265, 0.30954733, 0.31288338,\n",
       "       0.314     , 0.31642985, 0.32      , 0.320447  , 0.3263965 ,\n",
       "       0.329     , 0.33051243, 0.33063462, 0.331     , 0.332     ,\n",
       "       0.3373    , 0.3388026 , 0.3396    , 0.34      , 0.34      ,\n",
       "       0.3402    , 0.341     , 0.342     , 0.344     , 0.346     ,\n",
       "       0.348     , 0.348345  , 0.34858385, 0.352     , 0.355     ,\n",
       "       0.357     , 0.35750735, 0.3578    , 0.3600342 , 0.3619343 ,\n",
       "       0.363     , 0.3666029 , 0.368     , 0.369     , 0.3709    ,\n",
       "       0.374     , 0.3789658 , 0.3796623 , 0.38      , 0.3803595 ,\n",
       "       0.3804165 , 0.383     , 0.3872971 , 0.388     , 0.38928878,\n",
       "       0.3915992 , 0.39397448, 0.3965    , 0.399     , 0.39960134,\n",
       "       0.4       , 0.401     , 0.401     , 0.401     , 0.40245962,\n",
       "       0.4083191 , 0.41      , 0.412     , 0.415     , 0.416     ,\n",
       "       0.416     , 0.4209268 , 0.421     , 0.421     , 0.422     ,\n",
       "       0.423     , 0.425     , 0.426     , 0.429     , 0.43      ,\n",
       "       0.43      , 0.43      , 0.43      , 0.43      , 0.43      ,\n",
       "       0.436     , 0.436     , 0.44      , 0.44      , 0.449     ,\n",
       "       0.45      , 0.45      , 0.45      , 0.451     , 0.453     ,\n",
       "       0.455     , 0.46      , 0.4607    , 0.4627    , 0.463     ,\n",
       "       0.465     , 0.468     , 0.469     , 0.47      , 0.472     ,\n",
       "       0.475     , 0.477     , 0.479     , 0.48      , 0.49      ,\n",
       "       0.493     , 0.495     , 0.495     , 0.496     , 0.497     ,\n",
       "       0.497     , 0.498     , 0.5       , 0.5043    , 0.508     ,\n",
       "       0.51      , 0.511     , 0.514     , 0.519     , 0.521     ,\n",
       "       0.521     , 0.522     , 0.526     , 0.526     , 0.528     ,\n",
       "       0.528     , 0.532     , 0.539     , 0.54      , 0.54      ,\n",
       "       0.543     , 0.548     , 0.55      , 0.55      , 0.5516    ,\n",
       "       0.552     , 0.557     , 0.561     , 0.562     , 0.564     ,\n",
       "       0.568     , 0.57      , 0.57      , 0.571     , 0.579     ,\n",
       "       0.58      , 0.581     , 0.581     , 0.581     , 0.5817    ,\n",
       "       0.582     , 0.583     , 0.591     , 0.592     , 0.592     ,\n",
       "       0.599     , 0.603     , 0.604     , 0.61      , 0.612     ,\n",
       "       0.613     , 0.613     , 0.615     , 0.619     , 0.62      ,\n",
       "       0.62      , 0.623     , 0.6268    , 0.631     , 0.631     ,\n",
       "       0.633     , 0.633     , 0.64      , 0.64      , 0.64      ,\n",
       "       0.643     , 0.645     , 0.655     , 0.656     , 0.657     ,\n",
       "       0.67      , 0.671     , 0.679     , 0.68      , 0.687     ,\n",
       "       0.687     , 0.688     , 0.691     , 0.695     , 0.698     ,\n",
       "       0.707     , 0.71      , 0.711     , 0.721     , 0.73      ,\n",
       "       0.735     , 0.74      , 0.741     , 0.75      , 0.752     ,\n",
       "       0.756     , 0.763     , 0.772     , 0.78      , 0.781     ,\n",
       "       0.791     , 0.799     , 0.8       , 0.81      , 0.811     ,\n",
       "       0.812     , 0.815     , 0.816     , 0.817     , 0.818     ,\n",
       "       0.821     , 0.8218    , 0.83      , 0.83      , 0.833     ,\n",
       "       0.839     , 0.84      , 0.84      , 0.85      , 0.854     ,\n",
       "       0.859     , 0.86      , 0.868     , 0.87      , 0.874     ,\n",
       "       0.882     , 0.885     , 0.905     , 0.91      , 0.9271    ,\n",
       "       0.93      , 0.935     , 0.936     , 0.949     , 0.95      ,\n",
       "       0.95      , 0.95      , 0.953     , 0.96      , 0.961     ,\n",
       "       0.97      , 0.97      , 0.974     , 0.975     , 0.978     ,\n",
       "       0.983     , 1.01      , 1.01      , 1.017     , 1.02      ,\n",
       "       1.02      , 1.03      , 1.057     , 1.092     , 1.11      ,\n",
       "       1.12      , 1.124     , 1.14      , 1.14      , 1.188     ,\n",
       "       1.19      , 1.192     , 1.215     , 1.23      , 1.241     ,\n",
       "       1.265     , 1.3       , 1.305     , 1.307     , 1.315     ,\n",
       "       1.34      , 1.35      , 1.37      , 1.39      , 1.414     ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "redshift"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
