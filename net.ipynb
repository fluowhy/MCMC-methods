{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(2, 10)\n",
    "        self.fc2 = nn.Linear(10, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        x = softmax(x, dim=1, estable=False)\n",
    "        return x\n",
    "\n",
    "    \n",
    "class BatchingDataset():\n",
    "    def __init__(self, datos, clases, C):\n",
    "        self.C = C\n",
    "        self.datos = datos\n",
    "        self.N = len(self.datos)\n",
    "        self.clases = torch.FloatTensor(np.eye(self.N, self.C)[clases])\n",
    "        if torch.cuda.is_available():\n",
    "            self.datos = self.datos.type(torch.cuda.FloatTensor)\n",
    "            self.clases = self.clases.type(torch.cuda.FloatTensor)\n",
    "        return\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.N\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        #print(self.Td[0])\n",
    "        return self.datos[i]\n",
    "\n",
    "    def paquetes(self, B):\n",
    "        Tdb = []\n",
    "        Tcb = []\n",
    "        d = self.N%B\n",
    "        if d==0:\n",
    "            for i in range(0, self.N - 1, B):\n",
    "                Tdb.append(self.datos[i:i + B])\n",
    "                Tcb.append(self.clases[i:i + B])\n",
    "        else:\n",
    "            for i in range(0, self.N - d, B):\n",
    "                Tdb.append(self.datos[i:i + B])\n",
    "                Tcb.append(self.clases[i:i + B])\n",
    "            Tdb.append(self.datos[i + B:])\n",
    "            Tcb.append(self.clases[i + B:])\n",
    "        return zip(Tdb, Tcb)\n",
    "    \n",
    "    \n",
    "def train(net, dataset, optimizer, epochs, minibatches):\n",
    "    criterion = nn.MSELoss()\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "        running_loss = 0.0\n",
    "        i = 0\n",
    "        for x, y in dataset.paquetes(minibatches):\n",
    "            # get the inputs\n",
    "            inputs = x\n",
    "            labels = y\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = cross_ent_loss(outputs, y, epsilon=1e-10) # criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # print statistics\n",
    "            running_loss += loss.item()\n",
    "            i += 1\n",
    "        print('Loss', running_loss/i)       \n",
    "    print('Finished Training')\n",
    "    return\n",
    "\n",
    "\n",
    "def softmax(T, dim, estable=True):\n",
    "    if not estable:\n",
    "        m = torch.max(T, dim=dim, keepdim=True)[0].expand_as(T)\n",
    "        T_m = T - m\n",
    "        T_out = softmax(T_m, dim=dim, estable=True)\n",
    "        return T_out\n",
    "    elif estable:\n",
    "        T_exp = torch.exp(T)   \n",
    "        T_sum = torch.sum(T_exp, dim=dim, keepdim=True).expand_as(T)\n",
    "        T_out = T_exp*torch.reciprocal(T_sum)\n",
    "        T_out[T_out<1e-5] = 1e-5\n",
    "        T_out[T_out>1 - 1e-5] = 1 - 1e-5\n",
    "        if torch.cuda.is_available():\n",
    "            T_out = T_out.type(torch.cuda.FloatTensor)\n",
    "        return T_out\n",
    "    \n",
    "\n",
    "def cross_ent_loss(Q, P, epsilon=1e-10):\n",
    "    N = Q.shape[0]\n",
    "    Q[Q<epsilon] = epsilon\n",
    "    Q[Q>1 - epsilon] = 1 - epsilon\n",
    "    return - torch.sum(P.mul(torch.log(Q)))/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X tensor([[ 0.,  0.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.],\n",
      "        [ 1.,  1.]])\n",
      "clases tensor([[ 1.,  0.],\n",
      "        [ 0.,  1.],\n",
      "        [ 0.,  1.],\n",
      "        [ 1.,  0.]])\n"
     ]
    }
   ],
   "source": [
    "X = torch.FloatTensor(np.array([[0, 0], [0, 1], [1, 0], [1, 1]]))\n",
    "Y = torch.LongTensor(np.array([0, 1, 1, 0]))\n",
    "\n",
    "C = 2\n",
    "dataset = BatchingDataset(X, Y, C=C)\n",
    "\n",
    "print('X', dataset.datos)\n",
    "print('clases', dataset.clases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (fc1): Linear(in_features=2, out_features=10, bias=True)\n",
      "  (fc2): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "net = Net()\n",
    "net.zero_grad()\n",
    "print(net)\n",
    "\n",
    "# create your optimizer\n",
    "optimizer = optim.Adam(net.parameters(), lr=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.6979373395442963\n",
      "Loss 0.6873706877231598\n",
      "Loss 0.6809012591838837\n",
      "Loss 0.6764704287052155\n",
      "Loss 0.6734790802001953\n",
      "Loss 0.6706130504608154\n",
      "Loss 0.6676131784915924\n",
      "Loss 0.6636938750743866\n",
      "Loss 0.6598821878433228\n",
      "Loss 0.6558426320552826\n",
      "Loss 0.6516440808773041\n",
      "Loss 0.6474736928939819\n",
      "Loss 0.643253743648529\n",
      "Loss 0.6386961936950684\n",
      "Loss 0.6340575516223907\n",
      "Loss 0.6290125250816345\n",
      "Loss 0.6238309144973755\n",
      "Loss 0.6186474561691284\n",
      "Loss 0.6133686304092407\n",
      "Loss 0.6080304682254791\n",
      "Loss 0.6030366122722626\n",
      "Loss 0.5974973142147064\n",
      "Loss 0.5917837917804718\n",
      "Loss 0.5861423909664154\n",
      "Loss 0.5805787444114685\n",
      "Loss 0.5746118128299713\n",
      "Loss 0.5682806074619293\n",
      "Loss 0.5631172060966492\n",
      "Loss 0.55638587474823\n",
      "Loss 0.5501693338155746\n",
      "Loss 0.544768750667572\n",
      "Loss 0.5381458401679993\n",
      "Loss 0.5316604822874069\n",
      "Loss 0.5250028818845749\n",
      "Loss 0.5179801285266876\n",
      "Loss 0.511701375246048\n",
      "Loss 0.5051049590110779\n",
      "Loss 0.4980514943599701\n",
      "Loss 0.491300106048584\n",
      "Loss 0.4846945106983185\n",
      "Loss 0.4778027832508087\n",
      "Loss 0.47082188725471497\n",
      "Loss 0.46468740701675415\n",
      "Loss 0.4576213210821152\n",
      "Loss 0.4503725469112396\n",
      "Loss 0.4431985169649124\n",
      "Loss 0.43616919219493866\n",
      "Loss 0.42865851521492004\n",
      "Loss 0.4213746190071106\n",
      "Loss 0.4156237840652466\n",
      "Loss 0.40818987786769867\n",
      "Loss 0.4004858434200287\n",
      "Loss 0.39338254928588867\n",
      "Loss 0.3863326907157898\n",
      "Loss 0.37914544343948364\n",
      "Loss 0.3718514144420624\n",
      "Loss 0.3651625066995621\n",
      "Loss 0.3573153614997864\n",
      "Loss 0.35165636241436005\n",
      "Loss 0.3444613218307495\n",
      "Loss 0.3372834026813507\n",
      "Loss 0.3310696631669998\n",
      "Loss 0.32476377487182617\n",
      "Loss 0.3179561048746109\n",
      "Loss 0.3113216459751129\n",
      "Loss 0.3050490617752075\n",
      "Loss 0.2976451963186264\n",
      "Loss 0.2921760082244873\n",
      "Loss 0.2862576097249985\n",
      "Loss 0.2795071080327034\n",
      "Loss 0.2744087874889374\n",
      "Loss 0.26862818002700806\n",
      "Loss 0.2622664049267769\n",
      "Loss 0.25591983646154404\n",
      "Loss 0.24966168403625488\n",
      "Loss 0.24438068270683289\n",
      "Loss 0.2386007159948349\n",
      "Loss 0.23411396145820618\n",
      "Loss 0.22959299385547638\n",
      "Loss 0.2248523235321045\n",
      "Loss 0.21917244791984558\n",
      "Loss 0.21403422206640244\n",
      "Loss 0.2095986157655716\n",
      "Loss 0.2048092558979988\n",
      "Loss 0.20030122995376587\n",
      "Loss 0.19526700675487518\n",
      "Loss 0.19123579561710358\n",
      "Loss 0.18707437813282013\n",
      "Loss 0.18288177996873856\n",
      "Loss 0.1786552518606186\n",
      "Loss 0.1750931590795517\n",
      "Loss 0.17110690474510193\n",
      "Loss 0.1673237755894661\n",
      "Loss 0.16356977075338364\n",
      "Loss 0.15966112911701202\n",
      "Loss 0.1563023179769516\n",
      "Loss 0.15292564034461975\n",
      "Loss 0.14934460073709488\n",
      "Loss 0.1459731012582779\n",
      "Loss 0.14311657845973969\n",
      "Loss 0.13962975144386292\n",
      "Loss 0.1370217204093933\n",
      "Loss 0.1340646594762802\n",
      "Loss 0.131125558167696\n",
      "Loss 0.12858827412128448\n",
      "Loss 0.12589389830827713\n",
      "Loss 0.12313584238290787\n",
      "Loss 0.12058170884847641\n",
      "Loss 0.11826271191239357\n",
      "Loss 0.11571801081299782\n",
      "Loss 0.11318998783826828\n",
      "Loss 0.11088439077138901\n",
      "Loss 0.10873841866850853\n",
      "Loss 0.10669539123773575\n",
      "Loss 0.10440491139888763\n",
      "Loss 0.1023751012980938\n",
      "Loss 0.10032117366790771\n",
      "Loss 0.0986967533826828\n",
      "Loss 0.09642388671636581\n",
      "Loss 0.09465538337826729\n",
      "Loss 0.09308561682701111\n",
      "Loss 0.09126707911491394\n",
      "Loss 0.0895589180290699\n",
      "Loss 0.08802646771073341\n",
      "Loss 0.08627911284565926\n",
      "Loss 0.08467359468340874\n",
      "Loss 0.08321715891361237\n",
      "Loss 0.08141510561108589\n",
      "Loss 0.0800152812153101\n",
      "Loss 0.07861729525029659\n",
      "Loss 0.07700926437973976\n",
      "Loss 0.07584280520677567\n",
      "Loss 0.07468918338418007\n",
      "Loss 0.0733227301388979\n",
      "Loss 0.07209478318691254\n",
      "Loss 0.07091207802295685\n",
      "Loss 0.06960770674049854\n",
      "Loss 0.06857211329042912\n",
      "Loss 0.0675656758248806\n",
      "Loss 0.06638713553547859\n",
      "Loss 0.06533470749855042\n",
      "Loss 0.06434404477477074\n",
      "Loss 0.06325457990169525\n",
      "Loss 0.062247494235634804\n",
      "Loss 0.06134445406496525\n",
      "Loss 0.06041562743484974\n",
      "Loss 0.05949610285460949\n",
      "Loss 0.05861026979982853\n",
      "Loss 0.0576561838388443\n",
      "Loss 0.056844888255000114\n",
      "Loss 0.05600899085402489\n",
      "Loss 0.055279336869716644\n",
      "Loss 0.054379627108573914\n",
      "Loss 0.05354846268892288\n",
      "Loss 0.05287166126072407\n",
      "Loss 0.05209153704345226\n",
      "Loss 0.05124028027057648\n",
      "Loss 0.05059105157852173\n",
      "Loss 0.04996595159173012\n",
      "Loss 0.04914667829871178\n",
      "Loss 0.04847464710474014\n",
      "Loss 0.047747887670993805\n",
      "Loss 0.04711010120809078\n",
      "Loss 0.046576833352446556\n",
      "Loss 0.04579116031527519\n",
      "Loss 0.04529072903096676\n",
      "Loss 0.04464710596948862\n",
      "Loss 0.04408315848559141\n",
      "Loss 0.04349655099213123\n",
      "Loss 0.042854560539126396\n",
      "Loss 0.04245571978390217\n",
      "Loss 0.04174987971782684\n",
      "Loss 0.04132450185716152\n",
      "Loss 0.040776388719677925\n",
      "Loss 0.04021006077528\n",
      "Loss 0.0397577416151762\n",
      "Loss 0.03928271867334843\n",
      "Loss 0.03876503836363554\n",
      "Loss 0.03832326643168926\n",
      "Loss 0.03793206997215748\n",
      "Loss 0.03744598850607872\n",
      "Loss 0.03694043308496475\n",
      "Loss 0.03647245932370424\n",
      "Loss 0.03612276166677475\n",
      "Loss 0.03568927850574255\n",
      "Loss 0.0352373868227005\n",
      "Loss 0.03480108082294464\n",
      "Loss 0.034426162019371986\n",
      "Loss 0.03398720175027847\n",
      "Loss 0.03360592946410179\n",
      "Loss 0.03317957930266857\n",
      "Loss 0.03279346041381359\n",
      "Loss 0.032400619238615036\n",
      "Loss 0.03206286579370499\n",
      "Loss 0.03174776490777731\n",
      "Loss 0.03142116032540798\n",
      "Loss 0.03106684237718582\n",
      "Loss 0.030721580609679222\n",
      "Loss 0.03035058919340372\n",
      "Loss 0.03000043611973524\n",
      "Loss 0.029828619211912155\n",
      "Loss 0.029398323968052864\n",
      "Loss 0.029069500043988228\n",
      "Loss 0.028799649327993393\n",
      "Loss 0.0284868236631155\n",
      "Loss 0.02818327583372593\n",
      "Loss 0.027864744886755943\n",
      "Loss 0.027568642050027847\n",
      "Loss 0.027313292026519775\n",
      "Loss 0.027051907032728195\n",
      "Loss 0.026763935573399067\n",
      "Loss 0.026517106220126152\n",
      "Loss 0.026221824809908867\n",
      "Loss 0.02594721596688032\n",
      "Loss 0.02568166796118021\n",
      "Loss 0.025417101569473743\n",
      "Loss 0.025175035931169987\n",
      "Loss 0.02494307141751051\n",
      "Loss 0.024689939338713884\n",
      "Loss 0.024444912560284138\n",
      "Loss 0.024211326614022255\n",
      "Loss 0.023962879553437233\n",
      "Loss 0.023735509254038334\n",
      "Loss 0.02352837100625038\n",
      "Loss 0.02329295827075839\n",
      "Loss 0.0230665635317564\n",
      "Loss 0.02287320140749216\n",
      "Loss 0.022666560020297766\n",
      "Loss 0.022451652213931084\n",
      "Loss 0.022230714093893766\n",
      "Loss 0.022032877430319786\n",
      "Loss 0.021868870593607426\n",
      "Loss 0.021642942912876606\n",
      "Loss 0.02143421908840537\n",
      "Loss 0.021225868724286556\n",
      "Loss 0.021040525287389755\n",
      "Loss 0.02086436515673995\n",
      "Loss 0.02066585747525096\n",
      "Loss 0.020472653210163116\n",
      "Loss 0.020308634266257286\n",
      "Loss 0.02014684583991766\n",
      "Loss 0.019976425915956497\n",
      "Loss 0.019807113334536552\n",
      "Loss 0.019627024419605732\n",
      "Loss 0.01943549420684576\n",
      "Loss 0.019286522641777992\n",
      "Loss 0.019115960225462914\n",
      "Loss 0.018957692198455334\n",
      "Loss 0.01879128348082304\n",
      "Loss 0.01865194784477353\n",
      "Loss 0.01850465824827552\n",
      "Loss 0.018342720344662666\n",
      "Loss 0.01817245502024889\n",
      "Loss 0.01799801178276539\n",
      "Loss 0.017878747079521418\n",
      "Loss 0.01774028269574046\n",
      "Loss 0.017577096819877625\n",
      "Loss 0.01744559407234192\n",
      "Loss 0.017319283448159695\n",
      "Loss 0.017176509834825993\n",
      "Loss 0.0170263284817338\n",
      "Loss 0.01687096245586872\n",
      "Loss 0.016747683752328157\n",
      "Loss 0.016612064093351364\n",
      "Loss 0.01650054845958948\n",
      "Loss 0.016336651518940926\n",
      "Loss 0.01622047135606408\n",
      "Loss 0.016102980822324753\n",
      "Loss 0.015972510911524296\n",
      "Loss 0.01584264636039734\n",
      "Loss 0.015732188243418932\n",
      "Loss 0.015601743943989277\n",
      "Loss 0.015485020354390144\n",
      "Loss 0.015385587699711323\n",
      "Loss 0.015256485901772976\n",
      "Loss 0.01514092879369855\n",
      "Loss 0.015038399957120419\n",
      "Loss 0.014921312220394611\n",
      "Loss 0.014802910387516022\n",
      "Loss 0.014693121891468763\n",
      "Loss 0.014570988714694977\n",
      "Loss 0.014469400979578495\n",
      "Loss 0.014375293161720037\n",
      "Loss 0.014270603656768799\n",
      "Loss 0.014178141485899687\n",
      "Loss 0.014068007469177246\n",
      "Loss 0.013966347556561232\n",
      "Loss 0.013861371204257011\n",
      "Loss 0.013760571368038654\n",
      "Loss 0.01365289487875998\n",
      "Loss 0.013554628472775221\n",
      "Loss 0.013458380941301584\n",
      "Loss 0.013384288176894188\n",
      "Loss 0.013281156308948994\n",
      "Loss 0.01318687479943037\n",
      "Loss 0.013092068955302238\n",
      "Loss 0.01300546620041132\n",
      "Loss 0.012902243994176388\n",
      "Loss 0.012817999115213752\n",
      "Loss 0.012734295101836324\n",
      "Loss 0.012653157114982605\n",
      "Loss 0.012572959531098604\n",
      "Loss 0.012487049913033843\n",
      "Loss 0.012395786587148905\n",
      "Loss 0.012314543593674898\n",
      "Loss 0.012235095724463463\n",
      "Loss 0.01214690925553441\n",
      "Loss 0.012075786013156176\n",
      "Loss 0.011971503961831331\n",
      "Loss 0.011913398280739784\n",
      "Loss 0.011839677579700947\n",
      "Loss 0.011755403596907854\n",
      "Loss 0.011684648925438523\n",
      "Loss 0.01161078130826354\n",
      "Loss 0.011532267788425088\n",
      "Loss 0.011448309756815434\n",
      "Loss 0.011391071835532784\n",
      "Loss 0.011287648463621736\n",
      "Loss 0.011217128252610564\n",
      "Loss 0.011148098856210709\n",
      "Loss 0.011073842644691467\n",
      "Loss 0.011011949740350246\n",
      "Loss 0.010929615935310721\n",
      "Loss 0.010862317867577076\n",
      "Loss 0.010792532004415989\n",
      "Loss 0.010748504428192973\n",
      "Loss 0.010661507491022348\n",
      "Loss 0.010589116252958775\n",
      "Loss 0.010528573300689459\n",
      "Loss 0.010464737890288234\n",
      "Loss 0.010401599109172821\n",
      "Loss 0.010341545566916466\n",
      "Loss 0.010270195547491312\n",
      "Loss 0.010212395805865526\n",
      "Loss 0.010153095936402678\n",
      "Loss 0.010094155790284276\n",
      "Loss 0.01003666315227747\n",
      "Loss 0.009974290151149035\n",
      "Loss 0.009907720610499382\n",
      "Loss 0.009849904105067253\n",
      "Loss 0.009785458911210299\n",
      "Loss 0.009731479920446873\n",
      "Loss 0.00967258121818304\n",
      "Loss 0.009620883036404848\n",
      "Loss 0.009558916557580233\n",
      "Loss 0.009499611333012581\n",
      "Loss 0.009457668755203485\n",
      "Loss 0.009401814546436071\n",
      "Loss 0.009335780050605536\n",
      "Loss 0.009286878863349557\n",
      "Loss 0.00923642236739397\n",
      "Loss 0.00918155862018466\n",
      "Loss 0.009138969937339425\n",
      "Loss 0.009075029520317912\n",
      "Loss 0.009021481033414602\n",
      "Loss 0.008967202389612794\n",
      "Loss 0.00892621697857976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.008878047578036785\n",
      "Loss 0.008821832248941064\n",
      "Loss 0.008765466744080186\n",
      "Loss 0.008718457771465182\n",
      "Loss 0.008677069563418627\n",
      "Loss 0.008629438001662493\n",
      "Loss 0.008573864120990038\n",
      "Loss 0.008524381089955568\n",
      "Loss 0.008482993580400944\n",
      "Loss 0.008434317074716091\n",
      "Loss 0.00838547432795167\n",
      "Loss 0.008338244166225195\n",
      "Loss 0.008287932723760605\n",
      "Loss 0.008240605238825083\n",
      "Loss 0.008196973241865635\n",
      "Loss 0.008160875644534826\n",
      "Loss 0.008118528872728348\n",
      "Loss 0.00807542772963643\n",
      "Loss 0.00803133542649448\n",
      "Loss 0.007984193041920662\n",
      "Loss 0.0079439093824476\n",
      "Loss 0.007898212410509586\n",
      "Loss 0.007854873314499855\n",
      "Loss 0.007823324762284756\n",
      "Loss 0.007772351615130901\n",
      "Loss 0.007732800906524062\n",
      "Loss 0.007696221582591534\n",
      "Loss 0.007656612433493137\n",
      "Loss 0.007616066839545965\n",
      "Loss 0.007572404574602842\n",
      "Loss 0.007532512303441763\n",
      "Loss 0.007489615119993687\n",
      "Loss 0.0074523568619042635\n",
      "Loss 0.007411235012114048\n",
      "Loss 0.0073725475231185555\n",
      "Loss 0.007338583003729582\n",
      "Loss 0.007302322890609503\n",
      "Loss 0.007273518247529864\n",
      "Loss 0.007226161193102598\n",
      "Loss 0.007187126553617418\n",
      "Loss 0.007151445490308106\n",
      "Loss 0.00711383658926934\n",
      "Loss 0.0070877221878618\n",
      "Loss 0.0070451435167342424\n",
      "Loss 0.007011201698333025\n",
      "Loss 0.006974820280447602\n",
      "Loss 0.006938548060134053\n",
      "Loss 0.0069040944799780846\n",
      "Loss 0.006870473735034466\n",
      "Loss 0.006840734276920557\n",
      "Loss 0.006799536990001798\n",
      "Loss 0.006769999861717224\n",
      "Loss 0.006737612886354327\n",
      "Loss 0.006705944892019033\n",
      "Loss 0.006674292031675577\n",
      "Loss 0.006640861509367824\n",
      "Loss 0.006608780939131975\n",
      "Loss 0.006575369508937001\n",
      "Loss 0.006540904636494815\n",
      "Loss 0.006508567137643695\n",
      "Loss 0.0064765107817947865\n",
      "Loss 0.006451900117099285\n",
      "Loss 0.0064154257997870445\n",
      "Loss 0.006388113833963871\n",
      "Loss 0.0063599892891943455\n",
      "Loss 0.00632825936190784\n",
      "Loss 0.00630184612236917\n",
      "Loss 0.006267854478210211\n",
      "Loss 0.006237379973754287\n",
      "Loss 0.006207835860550404\n",
      "Loss 0.006178009090945125\n",
      "Loss 0.006162924924865365\n",
      "Loss 0.006117065786384046\n",
      "Loss 0.006090605398640037\n",
      "Loss 0.006062577944248915\n",
      "Loss 0.006036651320755482\n",
      "Loss 0.006007020128890872\n",
      "Loss 0.005977605935186148\n",
      "Loss 0.005949450423941016\n",
      "Loss 0.00592152657918632\n",
      "Loss 0.005899094860069454\n",
      "Loss 0.005868586013093591\n",
      "Loss 0.005845630774274468\n",
      "Loss 0.005821568425744772\n",
      "Loss 0.005797044257633388\n",
      "Loss 0.0057697700103744864\n",
      "Loss 0.005743161425925791\n",
      "Loss 0.00571247492916882\n",
      "Loss 0.00568513001780957\n",
      "Loss 0.005660088500007987\n",
      "Loss 0.005633588647469878\n",
      "Loss 0.005606103455647826\n",
      "Loss 0.005589132662862539\n",
      "Loss 0.005565313622355461\n",
      "Loss 0.005538499564863741\n",
      "Loss 0.005514904391020536\n",
      "Loss 0.005490400828421116\n",
      "Loss 0.005465867696329951\n",
      "Loss 0.005440199398435652\n",
      "Loss 0.00541215727571398\n",
      "Loss 0.005388078046962619\n",
      "Loss 0.00536957778967917\n",
      "Loss 0.00534823490306735\n",
      "Loss 0.005325202597305179\n",
      "Loss 0.005300167016685009\n",
      "Loss 0.005276229698210955\n",
      "Loss 0.005253178649581969\n",
      "Loss 0.005230152979493141\n",
      "Loss 0.005205322639085352\n",
      "Loss 0.00518050801474601\n",
      "Loss 0.005164181930013001\n",
      "Loss 0.0051363022066652775\n",
      "Loss 0.005115870153531432\n",
      "Loss 0.005096701672300696\n",
      "Loss 0.005075247259810567\n",
      "Loss 0.00505234650336206\n",
      "Loss 0.0050299037247896194\n",
      "Loss 0.0050111981108784676\n",
      "Loss 0.004986474057659507\n",
      "Loss 0.004966993001289666\n",
      "Loss 0.004950189962983131\n",
      "Loss 0.004928637528792024\n",
      "Loss 0.004909147159196436\n",
      "Loss 0.004888029070571065\n",
      "Loss 0.0048654170241206884\n",
      "Loss 0.00484684924595058\n",
      "Loss 0.00482905947137624\n",
      "Loss 0.004803162883035839\n",
      "Loss 0.0047850500559434295\n",
      "Loss 0.004763534641824663\n",
      "Loss 0.004743337165564299\n",
      "Loss 0.0047270681243389845\n",
      "Loss 0.004707470769062638\n",
      "Loss 0.004688380053266883\n",
      "Loss 0.0046706548891961575\n",
      "Loss 0.004649051232263446\n",
      "Loss 0.004629500675946474\n",
      "Loss 0.004609627882018685\n",
      "Loss 0.004593390738591552\n",
      "Loss 0.004573027836158872\n",
      "Loss 0.0045551786897704005\n",
      "Loss 0.004545516334474087\n",
      "Loss 0.004521153401583433\n",
      "Loss 0.004503388190641999\n",
      "Loss 0.004484385950490832\n",
      "Loss 0.004464721656404436\n",
      "Loss 0.004448017803952098\n",
      "Loss 0.00442933919839561\n",
      "Loss 0.004408744862303138\n",
      "Loss 0.004393492010422051\n",
      "Loss 0.004379752324894071\n",
      "Loss 0.00435881270095706\n",
      "Loss 0.004342162981629372\n",
      "Loss 0.004326121066696942\n",
      "Loss 0.004308977164328098\n",
      "Loss 0.0042924361769109964\n",
      "Loss 0.004275348968803883\n",
      "Loss 0.0042578044813126326\n",
      "Loss 0.004240409703925252\n",
      "Loss 0.00422421982511878\n",
      "Loss 0.004205344710499048\n",
      "Loss 0.004193961038254201\n",
      "Loss 0.004176444374024868\n",
      "Loss 0.004161933087743819\n",
      "Loss 0.004146469524130225\n",
      "Loss 0.0041296472772955894\n",
      "Loss 0.00411396287381649\n",
      "Loss 0.004098738310858607\n",
      "Loss 0.004081545281223953\n",
      "Loss 0.004063217900693417\n",
      "Loss 0.004049512092024088\n",
      "Loss 0.004033373203128576\n",
      "Loss 0.004020115127786994\n",
      "Loss 0.004011614364571869\n",
      "Loss 0.003994041820988059\n",
      "Loss 0.003979586414061487\n",
      "Loss 0.003963458933867514\n",
      "Loss 0.0039460924454033375\n",
      "Loss 0.003932331572286785\n",
      "Loss 0.003915945650078356\n",
      "Loss 0.003899641800671816\n",
      "Loss 0.0038897109334357083\n",
      "Loss 0.003870964632369578\n",
      "Loss 0.003857007366605103\n",
      "Loss 0.0038432596484199166\n",
      "Loss 0.003832478541880846\n",
      "Loss 0.0038172302884049714\n",
      "Loss 0.0038020052015781403\n",
      "Loss 0.003789289330597967\n",
      "Loss 0.0037745284498669207\n",
      "Loss 0.003758976934477687\n",
      "Loss 0.0037439889274537563\n",
      "Loss 0.003732027136720717\n",
      "Loss 0.0037194557953625917\n",
      "Loss 0.0037068742676638067\n",
      "Loss 0.0036945674219168723\n",
      "Loss 0.003680690540932119\n",
      "Loss 0.0036692096036858857\n",
      "Loss 0.0036517309490591288\n",
      "Loss 0.0036373037146404386\n",
      "Loss 0.0036274028243497014\n",
      "Loss 0.0036131825763732195\n",
      "Loss 0.003599950228817761\n",
      "Loss 0.003585545695386827\n",
      "Loss 0.003574452013708651\n",
      "Loss 0.003561461460776627\n",
      "Loss 0.0035497190547175705\n",
      "Loss 0.003535870462656021\n",
      "Loss 0.003521955746691674\n",
      "Loss 0.0035127706360071898\n",
      "Loss 0.003496919816825539\n",
      "Loss 0.003484320128336549\n",
      "Loss 0.0034728425089269876\n",
      "Loss 0.0034600532380864024\n",
      "Loss 0.0034488702658563852\n",
      "Loss 0.003437646257225424\n",
      "Loss 0.0034249298041686416\n",
      "Loss 0.003413478028960526\n",
      "Loss 0.0034000201267190278\n",
      "Loss 0.0033879316761158407\n",
      "Loss 0.0033758203499019146\n",
      "Loss 0.0033652630518190563\n",
      "Loss 0.003352043218910694\n",
      "Loss 0.003341683477628976\n",
      "Loss 0.0033299921778962016\n",
      "Loss 0.003319562063552439\n",
      "Loss 0.003308730956632644\n",
      "Loss 0.0032971682376228273\n",
      "Loss 0.003285897895693779\n",
      "Loss 0.0032729539088904858\n",
      "Loss 0.003261975245550275\n",
      "Loss 0.0032507606083527207\n",
      "Loss 0.003238762787077576\n",
      "Loss 0.003229700494557619\n",
      "Loss 0.0032173600629903376\n",
      "Loss 0.003207203932106495\n",
      "Loss 0.0031995585886761546\n",
      "Loss 0.003188499133102596\n",
      "Loss 0.0031769463093951344\n",
      "Loss 0.0031667017028667033\n",
      "Loss 0.0031554419547319412\n",
      "Loss 0.003144242102280259\n",
      "Loss 0.0031326100579462945\n",
      "Loss 0.003120321431197226\n",
      "Loss 0.003113683545961976\n",
      "Loss 0.003100957372225821\n",
      "Loss 0.0030930296634323895\n",
      "Loss 0.003081293252762407\n",
      "Loss 0.0030718737398274243\n",
      "Loss 0.003061078954488039\n",
      "Loss 0.0030498820124194026\n",
      "Loss 0.0030411335173994303\n",
      "Loss 0.003030483261682093\n",
      "Loss 0.0030196073930710554\n",
      "Loss 0.003008636995218694\n",
      "Loss 0.002999934833496809\n",
      "Loss 0.0029908749274909496\n",
      "Loss 0.002980874676723033\n",
      "Loss 0.0029731689719483256\n",
      "Loss 0.0029646556358784437\n",
      "Loss 0.002954264753498137\n",
      "Loss 0.002942577819339931\n",
      "Loss 0.002932847710326314\n",
      "Loss 0.0029228931525722146\n",
      "Loss 0.0029129989561624825\n",
      "Loss 0.002901314524933696\n",
      "Loss 0.0028953925939276814\n",
      "Loss 0.0028862839099019766\n",
      "Loss 0.0028767259791493416\n",
      "Loss 0.0028680125833489\n",
      "Loss 0.002859255706425756\n",
      "Loss 0.0028496908489614725\n",
      "Loss 0.002839724242221564\n",
      "Loss 0.0028293547802604735\n",
      "Loss 0.0028225999558344483\n",
      "Loss 0.0028129592537879944\n",
      "Loss 0.002802315284498036\n",
      "Loss 0.002793492458295077\n",
      "Loss 0.002785273944027722\n",
      "Loss 0.002776413457468152\n",
      "Loss 0.002766792254988104\n",
      "Loss 0.0027596156578511\n",
      "Loss 0.0027498401468619704\n",
      "Loss 0.0027421138947829604\n",
      "Loss 0.0027327575953677297\n",
      "Loss 0.0027238830225542188\n",
      "Loss 0.0027150989626534283\n",
      "Loss 0.0027064369060099125\n",
      "Loss 0.0026978744426742196\n",
      "Loss 0.0026920284144580364\n",
      "Loss 0.002683555823750794\n",
      "Loss 0.0026737188454717398\n",
      "Loss 0.0026657271082513034\n",
      "Loss 0.002656809869222343\n",
      "Loss 0.0026486089918762445\n",
      "Loss 0.0026394235901534557\n",
      "Loss 0.0026338965399190784\n",
      "Loss 0.0026223232271149755\n",
      "Loss 0.002617149497382343\n",
      "Loss 0.002608580281957984\n",
      "Loss 0.002601604792289436\n",
      "Loss 0.002594002871774137\n",
      "Loss 0.0025858638109639287\n",
      "Loss 0.0025772169465199113\n",
      "Loss 0.0025693370844237506\n",
      "Loss 0.0025605689734220505\n",
      "Loss 0.002552442834712565\n",
      "Loss 0.0025443045888096094\n",
      "Loss 0.002537108608521521\n",
      "Loss 0.0025326155591756105\n",
      "Loss 0.0025205472484230995\n",
      "Loss 0.0025132005685009062\n",
      "Loss 0.002508149715140462\n",
      "Loss 0.0025005173520185053\n",
      "Loss 0.0024914899258874357\n",
      "Loss 0.0024840394034981728\n",
      "Loss 0.0024761862587183714\n",
      "Loss 0.002467990037985146\n",
      "Loss 0.002459674491547048\n",
      "Loss 0.0024530222872272134\n",
      "Loss 0.0024475821992382407\n",
      "Loss 0.0024394483189098537\n",
      "Loss 0.0024328608997166157\n",
      "Loss 0.0024262922815978527\n",
      "Loss 0.002419216267298907\n",
      "Loss 0.0024114539846777916\n",
      "Loss 0.002403423306532204\n",
      "Loss 0.0023968892637640238\n",
      "Loss 0.0023891396122053266\n",
      "Loss 0.002381689613685012\n",
      "Loss 0.0023742267512716353\n",
      "Loss 0.0023697001161053777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss 0.0023607038310728967\n",
      "Loss 0.0023537189699709415\n",
      "Loss 0.0023478425573557615\n",
      "Loss 0.002341966493986547\n",
      "Loss 0.0023347852984443307\n",
      "Loss 0.002326973539311439\n",
      "Loss 0.0023203172022476792\n",
      "Loss 0.0023135271621868014\n",
      "Loss 0.002306020585820079\n",
      "Loss 0.0022982906666584313\n",
      "Loss 0.0022936304449103773\n",
      "Loss 0.0022854272392578423\n",
      "Loss 0.002279428648762405\n",
      "Loss 0.002273642341606319\n",
      "Loss 0.0022665212745778263\n",
      "Loss 0.002261258428916335\n",
      "Loss 0.0022533158771693707\n",
      "Loss 0.002246632124297321\n",
      "Loss 0.002240230212919414\n",
      "Loss 0.002233471372164786\n",
      "Loss 0.0022266380256041884\n",
      "Loss 0.0022211676696315408\n",
      "Loss 0.0022147621493786573\n",
      "Loss 0.0022091526770964265\n",
      "Loss 0.002202871721237898\n",
      "Loss 0.0021965187625028193\n",
      "Loss 0.002190206665545702\n",
      "Loss 0.0021835825173184276\n",
      "Loss 0.002177649294026196\n",
      "Loss 0.0021718157222494483\n",
      "Loss 0.0021651590359397233\n",
      "Loss 0.0021586104994639754\n",
      "Loss 0.0021532493410632014\n",
      "Loss 0.0021473689121194184\n",
      "Loss 0.0021409502369351685\n",
      "Loss 0.0021352226031012833\n",
      "Loss 0.0021294215694069862\n",
      "Loss 0.002123529266100377\n",
      "Loss 0.002117399068083614\n",
      "Loss 0.002110762055963278\n",
      "Loss 0.0021045124740339816\n",
      "Loss 0.0020994915394112468\n",
      "Loss 0.002092461916618049\n",
      "Loss 0.0020867986022494733\n",
      "Loss 0.002081292972434312\n",
      "Loss 0.0020767298992723227\n",
      "Loss 0.0020714208076242357\n",
      "Loss 0.0020654708496294916\n",
      "Loss 0.0020595208916347474\n",
      "Loss 0.0020543498685583472\n",
      "Loss 0.002047471993137151\n",
      "Loss 0.0020413430756889284\n",
      "Loss 0.0020360371854621917\n",
      "Loss 0.00202951836399734\n",
      "Loss 0.0020238837169017643\n",
      "Loss 0.002020104060648009\n",
      "Loss 0.002013565303059295\n",
      "Loss 0.0020098886161576957\n",
      "Loss 0.0020040106028318405\n",
      "Loss 0.001999238971620798\n",
      "Loss 0.0019943344814237207\n",
      "Loss 0.001988594653084874\n",
      "Loss 0.001982556248549372\n",
      "Loss 0.0019764734024647623\n",
      "Loss 0.001970152312424034\n",
      "Loss 0.0019635470234788954\n",
      "Loss 0.0019621965184342116\n",
      "Loss 0.001954506675247103\n",
      "Loss 0.0019502885697875172\n",
      "Loss 0.0019440805190242827\n",
      "Loss 0.001939057488925755\n",
      "Loss 0.001933855441166088\n",
      "Loss 0.0019281910499557853\n",
      "Loss 0.00192357535706833\n",
      "Loss 0.001918072230182588\n",
      "Loss 0.0019127497798763216\n",
      "Loss 0.0019073543371632695\n",
      "Loss 0.0019025270885322243\n",
      "Loss 0.0018977717263624072\n",
      "Loss 0.0018929284997284412\n",
      "Loss 0.0018875183013733476\n",
      "Loss 0.0018837994430214167\n",
      "Loss 0.0018774589407257736\n",
      "Loss 0.0018723922839853913\n",
      "Loss 0.0018677455664146692\n",
      "Loss 0.0018625568773131818\n",
      "Loss 0.0018575946451164782\n",
      "Loss 0.0018521254241932184\n",
      "Loss 0.0018469117349013686\n",
      "Loss 0.0018426341703161597\n",
      "Loss 0.0018382839625701308\n",
      "Loss 0.001833456481108442\n",
      "Loss 0.0018287189886905253\n",
      "Loss 0.0018239060300402343\n",
      "Loss 0.0018195727607235312\n",
      "Loss 0.0018140126776415855\n",
      "Loss 0.0018092454120051116\n",
      "Loss 0.0018046116747427732\n",
      "Loss 0.00179973995545879\n",
      "Loss 0.0017943907878361642\n",
      "Loss 0.0017901333631016314\n",
      "Loss 0.0017858558567240834\n",
      "Loss 0.001781789818778634\n",
      "Loss 0.0017774110601749271\n",
      "Loss 0.0017725691432133317\n",
      "Loss 0.0017681900062598288\n",
      "Loss 0.0017634080722928047\n",
      "Loss 0.0017585368768777698\n",
      "Loss 0.0017546096350997686\n",
      "Loss 0.0017496790969744325\n",
      "Loss 0.0017445667763240635\n",
      "Loss 0.0017401884542778134\n",
      "Loss 0.001735911937430501\n",
      "Loss 0.001732159813400358\n",
      "Loss 0.001727855793433264\n",
      "Loss 0.0017232386744581163\n",
      "Loss 0.0017184276948682964\n",
      "Loss 0.0017135723610408604\n",
      "Loss 0.0017089399916585535\n",
      "Loss 0.001704531692666933\n",
      "Loss 0.0017001551750581712\n",
      "Loss 0.0016958967607934028\n",
      "Loss 0.0016912483552005142\n",
      "Loss 0.001687678333837539\n",
      "Loss 0.0016829247470013797\n",
      "Loss 0.0016800571174826473\n",
      "Loss 0.0016754529788158834\n",
      "Loss 0.0016714318771846592\n",
      "Loss 0.0016673523932695389\n",
      "Loss 0.0016629895544610918\n",
      "Loss 0.0016587908612564206\n",
      "Loss 0.0016543534584343433\n",
      "Loss 0.001649498735787347\n",
      "Loss 0.001644375326577574\n",
      "Loss 0.001641316106542945\n",
      "Loss 0.0016386405332013965\n",
      "Loss 0.0016334507963620126\n",
      "Loss 0.0016301475407090038\n",
      "Loss 0.0016262326971627772\n",
      "Loss 0.0016222132253460586\n",
      "Loss 0.0016177914221771061\n",
      "Loss 0.0016134448815137148\n",
      "Loss 0.001609171915333718\n",
      "Loss 0.0016047049721237272\n",
      "Loss 0.0016005072102416307\n",
      "Loss 0.001596473070094362\n",
      "Loss 0.0015938616124913096\n",
      "Loss 0.0015883907326497138\n",
      "Loss 0.0015852973447181284\n",
      "Loss 0.0015818157698959112\n",
      "Loss 0.0015778558154124767\n",
      "Loss 0.0015740757808089256\n",
      "Loss 0.0015700124204158783\n",
      "Loss 0.0015666824474465102\n",
      "Loss 0.0015617516473867\n",
      "Loss 0.0015578821185044944\n",
      "Loss 0.001553923706524074\n",
      "Loss 0.001550100278109312\n",
      "Loss 0.0015465580218005925\n",
      "Loss 0.001543031947221607\n",
      "Loss 0.0015392822097055614\n",
      "Loss 0.0015354430070146918\n",
      "Loss 0.0015332045732066035\n",
      "Loss 0.001528691384010017\n",
      "Loss 0.0015248054405674338\n",
      "Loss 0.001521115453215316\n",
      "Loss 0.0015172466228250414\n",
      "Loss 0.001513407623860985\n",
      "Loss 0.0015092853573150933\n",
      "Loss 0.0015052235103212297\n",
      "Loss 0.001502160681411624\n",
      "Loss 0.001498454948887229\n",
      "Loss 0.0014952727360650897\n",
      "Loss 0.001491851347964257\n",
      "Loss 0.001488370558945462\n",
      "Loss 0.0014846213161945343\n",
      "Loss 0.0014814709429629147\n",
      "Loss 0.0014767054817639291\n",
      "Loss 0.0014736734447069466\n",
      "Loss 0.0014697143924422562\n",
      "Loss 0.001466114481445402\n",
      "Loss 0.0014642062014900148\n",
      "Loss 0.001459362916648388\n",
      "Loss 0.0014561506686732173\n",
      "Loss 0.001453147444408387\n",
      "Loss 0.001449726871214807\n",
      "Loss 0.0014465315325651318\n",
      "Loss 0.0014425426197703928\n",
      "Loss 0.0014389429416041821\n",
      "Loss 0.0014353142469190061\n",
      "Loss 0.0014325370430015028\n",
      "Loss 0.0014281595940701663\n",
      "Loss 0.0014247687649913132\n",
      "Loss 0.0014236520801205188\n",
      "Loss 0.0014178538986016065\n",
      "Loss 0.0014149104827083647\n",
      "Loss 0.0014117288519628346\n",
      "Loss 0.0014083831338211894\n",
      "Loss 0.001405262213665992\n",
      "Loss 0.0014016024069860578\n",
      "Loss 0.0013981672236695886\n",
      "Loss 0.001394657650962472\n",
      "Loss 0.001391566765960306\n",
      "Loss 0.0013877878081984818\n",
      "Loss 0.0013857581070624292\n",
      "Loss 0.0013818276347592473\n",
      "Loss 0.001379004359478131\n",
      "Loss 0.0013761211303062737\n",
      "Loss 0.0013731784711126238\n",
      "Loss 0.001369877834804356\n",
      "Loss 0.0013663388090208173\n",
      "Loss 0.00136341288452968\n",
      "Loss 0.001359752961434424\n",
      "Loss 0.0013566610869020224\n",
      "Loss 0.001353599305730313\n",
      "Loss 0.001350418257061392\n",
      "Loss 0.0013469687255565077\n",
      "Loss 0.0013454931904561818\n",
      "Loss 0.0013417130976449698\n",
      "Loss 0.0013376048300415277\n",
      "Loss 0.0013349919172469527\n",
      "Loss 0.0013318699784576893\n",
      "Loss 0.0013289870112203062\n",
      "Loss 0.001326104043982923\n",
      "Loss 0.0013229232863523066\n",
      "Loss 0.001319757429882884\n",
      "Loss 0.0013165767304599285\n",
      "Loss 0.0013131723389960825\n",
      "Loss 0.001310769934207201\n",
      "Loss 0.0013070794520899653\n",
      "Loss 0.00130467425333336\n",
      "Loss 0.0013019112520851195\n",
      "Loss 0.0013001200568396598\n",
      "Loss 0.0012961613829247653\n",
      "Loss 0.0012932494282722473\n",
      "Loss 0.0012900985893793404\n",
      "Loss 0.0012867989134974778\n",
      "Loss 0.001284366677282378\n",
      "Loss 0.001281289994949475\n",
      "Loss 0.0012778542004525661\n",
      "Loss 0.001275046612136066\n",
      "Loss 0.0012725820997729897\n",
      "Loss 0.0012698791397269815\n",
      "Loss 0.0012669522548094392\n",
      "Loss 0.0012638912303373218\n",
      "Loss 0.0012616825406439602\n",
      "Loss 0.001259338518138975\n",
      "Loss 0.0012554094137158245\n",
      "Loss 0.0012527959188446403\n",
      "Loss 0.001249973603989929\n",
      "Loss 0.0012469873763620853\n",
      "Loss 0.001243837148649618\n",
      "Loss 0.0012416285753715783\n",
      "Loss 0.0012383275316096842\n",
      "Loss 0.0012356540828477591\n",
      "Loss 0.001233040791703388\n",
      "Loss 0.0012305325362831354\n",
      "Loss 0.0012275606859475374\n",
      "Loss 0.0012247386039234698\n",
      "Loss 0.0012217823532409966\n",
      "Loss 0.0012195583840366453\n",
      "Loss 0.0012164674699306488\n",
      "Loss 0.0012133612763136625\n",
      "Loss 0.0012107781367376447\n",
      "Loss 0.0012087482609786093\n",
      "Loss 0.0012061495508532971\n",
      "Loss 0.0012032368686050177\n",
      "Loss 0.0012008326593786478\n",
      "Loss 0.001198085315991193\n",
      "Loss 0.001195622346131131\n",
      "Loss 0.001192665396956727\n",
      "Loss 0.0011902015539817512\n",
      "Loss 0.001187379821203649\n",
      "Loss 0.001184424210805446\n",
      "Loss 0.0011820205836556852\n",
      "Loss 0.0011791385477408767\n",
      "Loss 0.0011772579164244235\n",
      "Loss 0.0011744347284547985\n",
      "Loss 0.001172464049886912\n",
      "Loss 0.0011696117580868304\n",
      "Loss 0.0011672672699205577\n",
      "Loss 0.0011648039217106998\n",
      "Loss 0.0011621014564298093\n",
      "Loss 0.0011595633113756776\n",
      "Loss 0.0011568610789254308\n",
      "Loss 0.0011540843406692147\n",
      "Loss 0.001151143660536036\n",
      "Loss 0.001149054616689682\n",
      "Loss 0.0011473982594907284\n",
      "Loss 0.0011440961388871074\n",
      "Loss 0.0011420650698710233\n",
      "Loss 0.0011398997739888728\n",
      "Loss 0.0011374512978363782\n",
      "Loss 0.0011348387924954295\n",
      "Loss 0.0011322263162583113\n",
      "Loss 0.0011293901188764721\n",
      "Loss 0.0011274503485765308\n",
      "Loss 0.0011246883077546954\n",
      "Loss 0.0011218658473808318\n",
      "Loss 0.0011193279642611742\n",
      "Loss 0.0011175670952070504\n",
      "Loss 0.0011146400356665254\n",
      "Loss 0.0011124451993964612\n",
      "Loss 0.0011106542951893061\n",
      "Loss 0.001108220691094175\n",
      "Loss 0.001105592556996271\n",
      "Loss 0.0011033974878955632\n",
      "Loss 0.0011010536109097302\n",
      "Loss 0.0010989184374921024\n",
      "Loss 0.0010965149849653244\n",
      "Loss 0.0010938579798676074\n",
      "Finished Training\n"
     ]
    }
   ],
   "source": [
    "train(net, dataset, optimizer, epochs=1000, minibatches=2, show_each=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.,  2.],\n",
      "        [ 2.,  2.]]) tensor([[ 1.,  1.],\n",
      "        [ 1.,  1.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = torch.randn(2, 2, requires_grad=True)\n",
    "z = torch.sum(2*x + y)\n",
    "z.backward()uo\n",
    "print(x.grad, y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
